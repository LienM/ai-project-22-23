{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lecture 2: Introduction to Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas\n",
    "import pandas as pd\n",
    "import random\n",
    "\n",
    "import sklearn.preprocessing\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn import preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "articles = pd.read_csv('../data/articles.csv')\n",
    "customers = pd.read_csv('../data/customers.csv')\n",
    "sample_submisison = pd.read_csv('../data/sample_submission.csv')\n",
    "transactions = pd.read_csv('../data/transactions_train.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The H&M Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 105542 entries, 0 to 105541\n",
      "Data columns (total 25 columns):\n",
      " #   Column                        Non-Null Count   Dtype \n",
      "---  ------                        --------------   ----- \n",
      " 0   article_id                    105542 non-null  int64 \n",
      " 1   product_code                  105542 non-null  int64 \n",
      " 2   prod_name                     105542 non-null  object\n",
      " 3   product_type_no               105542 non-null  int64 \n",
      " 4   product_type_name             105542 non-null  object\n",
      " 5   product_group_name            105542 non-null  object\n",
      " 6   graphical_appearance_no       105542 non-null  int64 \n",
      " 7   graphical_appearance_name     105542 non-null  object\n",
      " 8   colour_group_code             105542 non-null  int64 \n",
      " 9   colour_group_name             105542 non-null  object\n",
      " 10  perceived_colour_value_id     105542 non-null  int64 \n",
      " 11  perceived_colour_value_name   105542 non-null  object\n",
      " 12  perceived_colour_master_id    105542 non-null  int64 \n",
      " 13  perceived_colour_master_name  105542 non-null  object\n",
      " 14  department_no                 105542 non-null  int64 \n",
      " 15  department_name               105542 non-null  object\n",
      " 16  index_code                    105542 non-null  object\n",
      " 17  index_name                    105542 non-null  object\n",
      " 18  index_group_no                105542 non-null  int64 \n",
      " 19  index_group_name              105542 non-null  object\n",
      " 20  section_no                    105542 non-null  int64 \n",
      " 21  section_name                  105542 non-null  object\n",
      " 22  garment_group_no              105542 non-null  int64 \n",
      " 23  garment_group_name            105542 non-null  object\n",
      " 24  detail_desc                   105126 non-null  object\n",
      "dtypes: int64(11), object(14)\n",
      "memory usage: 20.1+ MB\n"
     ]
    }
   ],
   "source": [
    "articles.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1371980 entries, 0 to 1371979\n",
      "Data columns (total 7 columns):\n",
      " #   Column                  Non-Null Count    Dtype  \n",
      "---  ------                  --------------    -----  \n",
      " 0   customer_id             1371980 non-null  object \n",
      " 1   FN                      476930 non-null   float64\n",
      " 2   Active                  464404 non-null   float64\n",
      " 3   club_member_status      1365918 non-null  object \n",
      " 4   fashion_news_frequency  1355971 non-null  object \n",
      " 5   age                     1356119 non-null  float64\n",
      " 6   postal_code             1371980 non-null  object \n",
      "dtypes: float64(3), object(4)\n",
      "memory usage: 73.3+ MB\n"
     ]
    }
   ],
   "source": [
    "customers.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 31788324 entries, 0 to 31788323\n",
      "Data columns (total 5 columns):\n",
      " #   Column            Dtype  \n",
      "---  ------            -----  \n",
      " 0   t_dat             object \n",
      " 1   customer_id       object \n",
      " 2   article_id        int64  \n",
      " 3   price             float64\n",
      " 4   sales_channel_id  int64  \n",
      "dtypes: float64(1), int64(2), object(2)\n",
      "memory usage: 1.2+ GB\n"
     ]
    }
   ],
   "source": [
    "transactions.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X = transactions.merge(customers, how='inner', on='customer_id')\n",
    "# X = X.merge(articles, how='inner', on='article_id')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating Samples \n",
    "If you would rather work with samples instead of the whole dataset (while prototyping your code). You can use the code below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.001\n",
      "0.01\n",
      "0.05\n"
     ]
    }
   ],
   "source": [
    "# Adapted from: https://www.kaggle.com/code/paweljankiewicz/hm-create-dataset-samples\n",
    "# This extracts three sampled datasets, containing 0.1%, 1% and 5% of all users and their transactions, and the associated articles.\n",
    "for sample_repr, sample in [(\"01\", 0.001), (\"1\", 0.01), (\"5\", 0.05)]:\n",
    "    print(sample)\n",
    "    customers_sample = customers.sample(int(customers.shape[0]*sample), replace=False)\n",
    "    customers_sample_ids = set(customers_sample[\"customer_id\"])\n",
    "    transactions_sample = transactions[transactions[\"customer_id\"].isin(customers_sample_ids)]\n",
    "    articles_sample_ids = set(transactions_sample[\"article_id\"])\n",
    "    articles_sample = articles[articles[\"article_id\"].isin(articles_sample_ids)]\n",
    "    customers_sample.to_csv(f\"../data/customers_sample{sample_repr}.csv.gz\", index=False)\n",
    "    transactions_sample.to_csv(f\"../data/transactions_sample{sample_repr}.csv.gz\", index=False)\n",
    "    articles_sample.to_csv(f\"../data/articles_sample{sample_repr}.csv.gz\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# articles_sample = pd.read_csv('../data/articles_sample01.csv.gz')\n",
    "# customers_sample = pd.read_csv('../data/customers_sample01.csv.gz')\n",
    "# transactions_sample = pd.read_csv('../data/transactions_sample01.csv.gz')\n",
    "articles_sample = pd.read_csv('../data/articles_sample5.csv.gz')\n",
    "customers_sample = pd.read_csv('../data/customers_sample5.csv.gz')\n",
    "transactions_sample = pd.read_csv('../data/transactions_sample5.csv.gz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 68599 entries, 0 to 68598\n",
      "Data columns (total 7 columns):\n",
      " #   Column                  Non-Null Count  Dtype  \n",
      "---  ------                  --------------  -----  \n",
      " 0   customer_id             68599 non-null  object \n",
      " 1   FN                      23976 non-null  float64\n",
      " 2   Active                  23373 non-null  float64\n",
      " 3   club_member_status      68327 non-null  object \n",
      " 4   fashion_news_frequency  67787 non-null  object \n",
      " 5   age                     67821 non-null  float64\n",
      " 6   postal_code             68599 non-null  object \n",
      "dtypes: float64(3), object(4)\n",
      "memory usage: 3.7+ MB\n"
     ]
    }
   ],
   "source": [
    "customers_sample.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1598761 entries, 0 to 1598760\n",
      "Data columns (total 5 columns):\n",
      " #   Column            Non-Null Count    Dtype  \n",
      "---  ------            --------------    -----  \n",
      " 0   t_dat             1598761 non-null  object \n",
      " 1   customer_id       1598761 non-null  object \n",
      " 2   article_id        1598761 non-null  int64  \n",
      " 3   price             1598761 non-null  float64\n",
      " 4   sales_channel_id  1598761 non-null  int64  \n",
      "dtypes: float64(1), int64(2), object(2)\n",
      "memory usage: 61.0+ MB\n"
     ]
    }
   ],
   "source": [
    "transactions_sample.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A Simplified Task: Binary Classification\n",
    "\n",
    "The task of predicting which 12 items users are most likely to buy in the next week is difficult to translate to a traditional classification machine learning setting. \n",
    "To obtain the 12 items a user is most likely to buy, we need to make predictions for all items (or the ones selected by a baseline) and select the 12 that have the highest predicted scores.\n",
    "\n",
    "In this assignment, we'll consider a simplified task: Predict whether a user ordered a single item or not, based on the features of the user and the item. \n",
    "We provide a baseline logistic regression model below, but haven't done much feature preprocessing or engineering!\n",
    "Initially, it is always best to focus your efforts on getting your features in the right shape and setting up the right validation scheme and baselines.\n",
    "Once you are sure that your features add value and your validation scheme is correct, then you typically move on to trying more elaborate models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you'd rather use a sample. Uncomment the following code:\n",
    "transactions = transactions_sample\n",
    "customers = customers_sample\n",
    "articles = articles_sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "transactions['ordered'] = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The problem setting is an example of a \"PU learning\" problem, i.e. only positives are labeled, everything else is unlabeled (and can be either positive or negative). \n",
    "Of course, we cannot train a classifier with just positive samples: The classifier will just learn that everything is positive.\n",
    "Therefore, we need to manually generate negative samples.\n",
    "\n",
    "Below, we use a simple random negative sampling strategy.\n",
    "We want to create a balanced dataset, meaning that we have just as many positives as negatives.\n",
    "This makes sure that the classifier will not benefit from predicting the positive/negative class more often than the other.\n",
    "Realistically, the amount of positive samples is of course many times smaller than the amount of unlabeled, possibly negative instances.\n",
    "\n",
    "\n",
    "If you want to try your hand at a more complex negative sampling strategy, you may want to check out this blog as a starting point: https://medium.com/mlearning-ai/overview-negative-sampling-on-recommendation-systems-230a051c6cd7.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "        t_dat                                        customer_id  article_id  \\\n0  2018-09-20  000aa7f0dc06cd7174389e76c9e132a67860c5f65f9706...   501820043   \n1  2018-09-20  000aa7f0dc06cd7174389e76c9e132a67860c5f65f9706...   501820043   \n2  2018-09-20  000aa7f0dc06cd7174389e76c9e132a67860c5f65f9706...   674681001   \n3  2018-09-20  000aa7f0dc06cd7174389e76c9e132a67860c5f65f9706...   671505001   \n4  2018-09-20  000aa7f0dc06cd7174389e76c9e132a67860c5f65f9706...   671505001   \n\n      price  sales_channel_id  ordered  \n0  0.016932                 2        1  \n1  0.016932                 2        1  \n2  0.008458                 2        1  \n3  0.033881                 2        1  \n4  0.033881                 2        1  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>t_dat</th>\n      <th>customer_id</th>\n      <th>article_id</th>\n      <th>price</th>\n      <th>sales_channel_id</th>\n      <th>ordered</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>2018-09-20</td>\n      <td>000aa7f0dc06cd7174389e76c9e132a67860c5f65f9706...</td>\n      <td>501820043</td>\n      <td>0.016932</td>\n      <td>2</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2018-09-20</td>\n      <td>000aa7f0dc06cd7174389e76c9e132a67860c5f65f9706...</td>\n      <td>501820043</td>\n      <td>0.016932</td>\n      <td>2</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2018-09-20</td>\n      <td>000aa7f0dc06cd7174389e76c9e132a67860c5f65f9706...</td>\n      <td>674681001</td>\n      <td>0.008458</td>\n      <td>2</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>2018-09-20</td>\n      <td>000aa7f0dc06cd7174389e76c9e132a67860c5f65f9706...</td>\n      <td>671505001</td>\n      <td>0.033881</td>\n      <td>2</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>2018-09-20</td>\n      <td>000aa7f0dc06cd7174389e76c9e132a67860c5f65f9706...</td>\n      <td>671505001</td>\n      <td>0.033881</td>\n      <td>2</td>\n      <td>1</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transactions.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# What's happening here? \n",
    "# We're creating negative samples. I.e. we're creating transactions that didn't actually occur.\n",
    "# First, we need to know which interactions did occur:\n",
    "positive_pairs = list(map(tuple, transactions[['customer_id', 'article_id']].drop_duplicates().values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "        t_dat                                        customer_id  article_id  \\\n0  2018-09-20  000aa7f0dc06cd7174389e76c9e132a67860c5f65f9706...   501820043   \n1  2018-09-20  000aa7f0dc06cd7174389e76c9e132a67860c5f65f9706...   501820043   \n2  2018-09-20  000aa7f0dc06cd7174389e76c9e132a67860c5f65f9706...   674681001   \n3  2018-09-20  000aa7f0dc06cd7174389e76c9e132a67860c5f65f9706...   671505001   \n4  2018-09-20  000aa7f0dc06cd7174389e76c9e132a67860c5f65f9706...   671505001   \n\n      price  sales_channel_id  ordered  \n0  0.016932                 2        1  \n1  0.016932                 2        1  \n2  0.008458                 2        1  \n3  0.033881                 2        1  \n4  0.033881                 2        1  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>t_dat</th>\n      <th>customer_id</th>\n      <th>article_id</th>\n      <th>price</th>\n      <th>sales_channel_id</th>\n      <th>ordered</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>2018-09-20</td>\n      <td>000aa7f0dc06cd7174389e76c9e132a67860c5f65f9706...</td>\n      <td>501820043</td>\n      <td>0.016932</td>\n      <td>2</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2018-09-20</td>\n      <td>000aa7f0dc06cd7174389e76c9e132a67860c5f65f9706...</td>\n      <td>501820043</td>\n      <td>0.016932</td>\n      <td>2</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2018-09-20</td>\n      <td>000aa7f0dc06cd7174389e76c9e132a67860c5f65f9706...</td>\n      <td>674681001</td>\n      <td>0.008458</td>\n      <td>2</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>2018-09-20</td>\n      <td>000aa7f0dc06cd7174389e76c9e132a67860c5f65f9706...</td>\n      <td>671505001</td>\n      <td>0.033881</td>\n      <td>2</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>2018-09-20</td>\n      <td>000aa7f0dc06cd7174389e76c9e132a67860c5f65f9706...</td>\n      <td>671505001</td>\n      <td>0.033881</td>\n      <td>2</td>\n      <td>1</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Then we need to know what every synthetic transaction should contain: a date, a customer_id, an article_id, price, sales_channel_id. We will set ordered = 0, as these transactions didn't really occur.\n",
    "transactions.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract real values\n",
    "real_dates = transactions[\"t_dat\"].unique()\n",
    "real_customers = transactions[\"customer_id\"].unique()\n",
    "real_articles = transactions[\"article_id\"].unique()\n",
    "real_channels = transactions[\"sales_channel_id\"].unique()\n",
    "article_and_price = transactions[[\"article_id\",\"price\"]].drop_duplicates(\"article_id\").set_index(\"article_id\").squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1598761\n"
     ]
    }
   ],
   "source": [
    "# How many negatives do we need to sample?\n",
    "num_neg_pos = transactions.shape[0]\n",
    "print(num_neg_pos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sampling negatives by selecting random users, articles, dates and sales channel:\n",
    "# Note: This is quite naive. Some articles may not even have been available at the date we are sampling.\n",
    "random.seed(42)\n",
    "\n",
    "# Afterwards, we need to remove potential duplicates, so we'll sample too many.\n",
    "num_neg_samples = int(num_neg_pos * 1.1)\n",
    "\n",
    "# Sample each of the independent attributes.\n",
    "neg_dates = np.random.choice(real_dates, size=num_neg_samples)\n",
    "neg_articles = np.random.choice(real_articles, size=num_neg_samples)\n",
    "neg_customers = np.random.choice(real_customers, size=num_neg_samples)\n",
    "neg_channels = np.random.choice(real_channels, size=num_neg_samples)\n",
    "ordered = np.array([0] * num_neg_samples)\n",
    "# Assign to every article a real price.\n",
    "neg_prices = article_and_price[neg_articles].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "neg_transactions = pd.DataFrame([neg_dates, neg_customers, neg_articles, neg_prices, neg_channels, ordered], index=transactions.columns).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "        t_dat                                        customer_id article_id  \\\n0  2019-06-18  91d2ea22258e751d8df43b6df0eb0dc6c63ed2b7e715e6...  668043001   \n1  2020-04-07  6fcaa1e92c6ac894352487838e53b73a2eb10d8b108bb9...  863002001   \n2  2020-02-10  be8ee14e5835aed177c1fa318b4c26f83508e8e4ef48d3...  583463001   \n3  2019-04-25  e221e4aa9cc3d7e47e345a831c451879e5f04035bbac8c...  529012040   \n4  2019-03-12  ff83781276eb89f8f6fb08848ec1dc6c39cd3c341eaf62...  890010001   \n\n      price sales_channel_id ordered  \n0  0.042356                1       0  \n1  0.042356                1       0  \n2  0.042356                2       0  \n3  0.010153                2       0  \n4  0.028797                1       0  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>t_dat</th>\n      <th>customer_id</th>\n      <th>article_id</th>\n      <th>price</th>\n      <th>sales_channel_id</th>\n      <th>ordered</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>2019-06-18</td>\n      <td>91d2ea22258e751d8df43b6df0eb0dc6c63ed2b7e715e6...</td>\n      <td>668043001</td>\n      <td>0.042356</td>\n      <td>1</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2020-04-07</td>\n      <td>6fcaa1e92c6ac894352487838e53b73a2eb10d8b108bb9...</td>\n      <td>863002001</td>\n      <td>0.042356</td>\n      <td>1</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2020-02-10</td>\n      <td>be8ee14e5835aed177c1fa318b4c26f83508e8e4ef48d3...</td>\n      <td>583463001</td>\n      <td>0.042356</td>\n      <td>2</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>2019-04-25</td>\n      <td>e221e4aa9cc3d7e47e345a831c451879e5f04035bbac8c...</td>\n      <td>529012040</td>\n      <td>0.010153</td>\n      <td>2</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>2019-03-12</td>\n      <td>ff83781276eb89f8f6fb08848ec1dc6c39cd3c341eaf62...</td>\n      <td>890010001</td>\n      <td>0.028797</td>\n      <td>1</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Result:\n",
    "neg_transactions.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "(1758637, 6)"
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "neg_transactions.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove random negative samples that actually coincide with positives\n",
    "df = neg_transactions[\n",
    "    ~neg_transactions.set_index([\"customer_id\", \"article_id\"]).index.isin(positive_pairs)\n",
    "]\n",
    "\n",
    "# Remove any excess\n",
    "chosen_neg_transactions = df.sample(num_neg_pos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concat the negative samples to the positive samples:\n",
    "transactions = pd.concat([transactions, chosen_neg_transactions])\n",
    "transactions = transactions.merge(customers, how=\"inner\", on='customer_id')\n",
    "transactions = transactions.merge(articles, how=\"inner\", on='article_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 3197522 entries, 0 to 3197521\n",
      "Data columns (total 36 columns):\n",
      " #   Column                        Dtype  \n",
      "---  ------                        -----  \n",
      " 0   t_dat                         object \n",
      " 1   customer_id                   object \n",
      " 2   article_id                    object \n",
      " 3   price                         object \n",
      " 4   sales_channel_id              object \n",
      " 5   ordered                       object \n",
      " 6   FN                            float64\n",
      " 7   Active                        float64\n",
      " 8   club_member_status            object \n",
      " 9   fashion_news_frequency        object \n",
      " 10  age                           float64\n",
      " 11  postal_code                   object \n",
      " 12  product_code                  int64  \n",
      " 13  prod_name                     object \n",
      " 14  product_type_no               int64  \n",
      " 15  product_type_name             object \n",
      " 16  product_group_name            object \n",
      " 17  graphical_appearance_no       int64  \n",
      " 18  graphical_appearance_name     object \n",
      " 19  colour_group_code             int64  \n",
      " 20  colour_group_name             object \n",
      " 21  perceived_colour_value_id     int64  \n",
      " 22  perceived_colour_value_name   object \n",
      " 23  perceived_colour_master_id    int64  \n",
      " 24  perceived_colour_master_name  object \n",
      " 25  department_no                 int64  \n",
      " 26  department_name               object \n",
      " 27  index_code                    object \n",
      " 28  index_name                    object \n",
      " 29  index_group_no                int64  \n",
      " 30  index_group_name              object \n",
      " 31  section_no                    int64  \n",
      " 32  section_name                  object \n",
      " 33  garment_group_no              int64  \n",
      " 34  garment_group_name            object \n",
      " 35  detail_desc                   object \n",
      "dtypes: float64(3), int64(10), object(23)\n",
      "memory usage: 902.6+ MB\n"
     ]
    }
   ],
   "source": [
    "transactions.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basic Preprocessing\n",
    "Some very basic preprocessing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "                                         customer_id   age article_id  \\\n0  000aa7f0dc06cd7174389e76c9e132a67860c5f65f9706...  22.0  501820043   \n1  000aa7f0dc06cd7174389e76c9e132a67860c5f65f9706...  22.0  501820043   \n2  12edefb1c3465468dc8f074038d824df183f847a7d1bf1...  61.0  501820043   \n3  62cd1ad9cde544e046f48bec6b91e82fdffeb37ee86d02...  31.0  501820043   \n4  fd639feddd7afb7ecd979ce54b412856e5d9a113b69329...  21.0  501820043   \n\n  sales_channel_id     price ordered  \n0                2  0.016932       1  \n1                2  0.016932       1  \n2                2  0.016932       0  \n3                1  0.013559       1  \n4                2  0.015237       1  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>customer_id</th>\n      <th>age</th>\n      <th>article_id</th>\n      <th>sales_channel_id</th>\n      <th>price</th>\n      <th>ordered</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>000aa7f0dc06cd7174389e76c9e132a67860c5f65f9706...</td>\n      <td>22.0</td>\n      <td>501820043</td>\n      <td>2</td>\n      <td>0.016932</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>000aa7f0dc06cd7174389e76c9e132a67860c5f65f9706...</td>\n      <td>22.0</td>\n      <td>501820043</td>\n      <td>2</td>\n      <td>0.016932</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>12edefb1c3465468dc8f074038d824df183f847a7d1bf1...</td>\n      <td>61.0</td>\n      <td>501820043</td>\n      <td>2</td>\n      <td>0.016932</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>62cd1ad9cde544e046f48bec6b91e82fdffeb37ee86d02...</td>\n      <td>31.0</td>\n      <td>501820043</td>\n      <td>1</td>\n      <td>0.013559</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>fd639feddd7afb7ecd979ce54b412856e5d9a113b69329...</td>\n      <td>21.0</td>\n      <td>501820043</td>\n      <td>2</td>\n      <td>0.015237</td>\n      <td>1</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# I'm dropping a lot of columns, use them in your engineering tasks!\n",
    "transactions_processed = transactions[['customer_id', 'age', 'article_id', 'sales_channel_id', 'price', 'ordered']].copy()\n",
    "transactions_processed.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Does it make sense to label encode?\n",
    "# Label encoding the customer and article IDs:\n",
    "customer_encoder = preprocessing.LabelEncoder()\n",
    "customer_encoder = customer_encoder.fit(transactions_processed['customer_id'])\n",
    "article_encoder = preprocessing.LabelEncoder()\n",
    "article_encoder = article_encoder.fit(transactions_processed['article_id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "transactions_processed['customer_id'] = customer_encoder.transform(transactions_processed['customer_id'])\n",
    "transactions_processed['article_id'] = article_encoder.transform(transactions_processed['article_id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "array(['000114c6996ef5703a8d455faf2103f8488d3928348e0739c92ff9e8897932db'],\n      dtype=object)"
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# If you want to go back to the original encoding:\n",
    "customer_encoder.inverse_transform([2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "   customer_id   age  article_id sales_channel_id     price ordered\n0           14  22.0        4029                2  0.016932       1\n1           14  22.0        4029                2  0.016932       1\n2         5052  61.0        4029                2  0.016932       0\n3        26311  31.0        4029                1  0.013559       1\n4        67468  21.0        4029                2  0.015237       1",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>customer_id</th>\n      <th>age</th>\n      <th>article_id</th>\n      <th>sales_channel_id</th>\n      <th>price</th>\n      <th>ordered</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>14</td>\n      <td>22.0</td>\n      <td>4029</td>\n      <td>2</td>\n      <td>0.016932</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>14</td>\n      <td>22.0</td>\n      <td>4029</td>\n      <td>2</td>\n      <td>0.016932</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>5052</td>\n      <td>61.0</td>\n      <td>4029</td>\n      <td>2</td>\n      <td>0.016932</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>26311</td>\n      <td>31.0</td>\n      <td>4029</td>\n      <td>1</td>\n      <td>0.013559</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>67468</td>\n      <td>21.0</td>\n      <td>4029</td>\n      <td>2</td>\n      <td>0.015237</td>\n      <td>1</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transactions_processed.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "False"
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Can you come up with a NaN strategy that makes sense for each column in the dataset?\n",
    "# Imputing all NaN values with zeros:\n",
    "transactions_processed = transactions_processed.fillna(0)\n",
    "transactions_processed.isnull().values.any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Does it make sense to one-hot encode?\n",
    "# One-hot-encoding sales_channel_id:\n",
    "transactions_processed = pd.get_dummies(transactions_processed, columns=['sales_channel_id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "   customer_id   age  article_id     price  ordered  sales_channel_id_1  \\\n0           14  22.0        4029  0.016932        1                   0   \n1           14  22.0        4029  0.016932        1                   0   \n2         5052  61.0        4029  0.016932        0                   0   \n3        26311  31.0        4029  0.013559        1                   1   \n4        67468  21.0        4029  0.015237        1                   0   \n\n   sales_channel_id_2  \n0                   1  \n1                   1  \n2                   1  \n3                   0  \n4                   1  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>customer_id</th>\n      <th>age</th>\n      <th>article_id</th>\n      <th>price</th>\n      <th>ordered</th>\n      <th>sales_channel_id_1</th>\n      <th>sales_channel_id_2</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>14</td>\n      <td>22.0</td>\n      <td>4029</td>\n      <td>0.016932</td>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>14</td>\n      <td>22.0</td>\n      <td>4029</td>\n      <td>0.016932</td>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>5052</td>\n      <td>61.0</td>\n      <td>4029</td>\n      <td>0.016932</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>26311</td>\n      <td>31.0</td>\n      <td>4029</td>\n      <td>0.013559</td>\n      <td>1</td>\n      <td>1</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>67468</td>\n      <td>21.0</td>\n      <td>4029</td>\n      <td>0.015237</td>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transactions_processed.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a Train / Test Split:\n",
    "X_train, X_test, y_train, y_test = train_test_split(transactions_processed.drop('ordered', axis=1), transactions_processed['ordered'], test_size=0.10, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "         customer_id   age  article_id     price  sales_channel_id_1  \\\n2569994        28869   0.0       34102  0.011847                   0   \n1288573        51546  48.0       76420  0.024339                   1   \n2826712        58580  19.0       50448  0.010661                   0   \n297566         37155  22.0       68707  0.016780                   1   \n1530737         3494  46.0       57591  0.025407                   1   \n\n         sales_channel_id_2  \n2569994                   1  \n1288573                   0  \n2826712                   1  \n297566                    0  \n1530737                   0  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>customer_id</th>\n      <th>age</th>\n      <th>article_id</th>\n      <th>price</th>\n      <th>sales_channel_id_1</th>\n      <th>sales_channel_id_2</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>2569994</th>\n      <td>28869</td>\n      <td>0.0</td>\n      <td>34102</td>\n      <td>0.011847</td>\n      <td>0</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>1288573</th>\n      <td>51546</td>\n      <td>48.0</td>\n      <td>76420</td>\n      <td>0.024339</td>\n      <td>1</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2826712</th>\n      <td>58580</td>\n      <td>19.0</td>\n      <td>50448</td>\n      <td>0.010661</td>\n      <td>0</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>297566</th>\n      <td>37155</td>\n      <td>22.0</td>\n      <td>68707</td>\n      <td>0.016780</td>\n      <td>1</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1530737</th>\n      <td>3494</td>\n      <td>46.0</td>\n      <td>57591</td>\n      <td>0.025407</td>\n      <td>1</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "2569994    0\n1288573    0\n2826712    0\n297566     1\n1530737    1\nName: ordered, dtype: int64"
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Will take a few minutes to run, if you're using the whole dataset:\n",
    "baseline = LogisticRegression(random_state=42, n_jobs=6)\n",
    "baseline = baseline.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "array([[0.4167443 , 0.5832557 ],\n       [0.63262432, 0.36737568],\n       [0.40244592, 0.59755408],\n       ...,\n       [0.61521631, 0.38478369],\n       [0.40444053, 0.59555947],\n       [0.63203481, 0.36796519]])"
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "baseline.predict_proba(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": "3095538    1\n673337     0\n1928549    0\n302201     1\n839103     1\n          ..\n153997     1\n1795664    0\n1452521    0\n2044666    0\n2788098    0\nName: ordered, Length: 319753, dtype: int64"
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "0.6011671508945967"
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Mean Accuracy:\n",
    "baseline.score(X_test, y_test)\n",
    "# As you can seen, the accuracy is ~0.51. In other words, the classifier predicts correctly 51% of the time whether a customer did or din't buy an item.\n",
    "# Can you improve this baseline logistic regression model by doing better preprocessing and generating new features?\n",
    "# Also think about my steps! Did it make sense to include the article and customer ids? (And things like that)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.63      0.50      0.55    159499\n",
      "           1       0.58      0.70      0.64    160254\n",
      "\n",
      "    accuracy                           0.60    319753\n",
      "   macro avg       0.61      0.60      0.60    319753\n",
      "weighted avg       0.61      0.60      0.60    319753\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Classification Metrics:\n",
    "predictions = baseline.predict(X_test)\n",
    "print(classification_report(y_test, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "array([1, 0, 1, ..., 0, 1, 0])"
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assignment: Feature engineering\n",
    "**TODO:** \n",
    "- In groups (of 2-3 students), think about (a few) features that can be engineered (preprocess and generate new features). Divide the work!\n",
    "- Do these engineered features improve the baseline model?\n",
    "- Add your thoughts & results to a slide deck for discussion next week (again, 1 slide per person).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# prod_name Word2Vec\n",
    "Evaluation may or may not work. Copying the code to a py file works more reliably.\n",
    "Reducing vector size may work (but give worse performance).\n",
    "\n",
    "\n",
    "The Following blocks of code were originally run in a .py file (for stability/easier debugging) and copied over to the notebook."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "outputs": [],
   "source": [
    "import gensim\n",
    "import string\n",
    "from nltk import word_tokenize\n",
    "def evaluate(frame, vec_size=50, extra_vec=None):\n",
    "    \"\"\"\n",
    "    Evaluates frame using LogisticRegression\n",
    "    \"\"\"\n",
    "    print(\"Evaluating...\")\n",
    "    frame = frame.copy()\n",
    "    seed = 42\n",
    "    frame = frame.drop(['customer_id', 'article_id'], axis=1)\n",
    "    scaler = preprocessing.StandardScaler()\n",
    "    # scaler = preprocessing.MinMaxScaler()\n",
    "    frame[['age', 'price']] = scaler.fit_transform(frame[['age', 'price']])\n",
    "    if vec_size > 0:\n",
    "        vec_idx = range(vec_size)\n",
    "        frame[vec_idx] = scaler.fit_transform(frame[vec_idx])\n",
    "    if extra_vec is not None:\n",
    "        frame[extra_vec] = scaler.fit_transform(frame[extra_vec])\n",
    "    print(frame)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(frame.drop('ordered', axis=1), frame['ordered'], test_size=0.10,\n",
    "                                                        random_state=seed)\n",
    "    # Will take a few minutes to run, if you're using the whole dataset:\n",
    "    better = LogisticRegression(random_state=seed, n_jobs=7, verbose=False, max_iter=1000)\n",
    "    better = better.fit(X_train, y_train)\n",
    "    better.predict_proba(X_test)\n",
    "    better.score(X_test, y_test)\n",
    "    better_predictions = better.predict(X_test)\n",
    "    print(classification_report(y_test, better_predictions))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "vec_size1 = 50\n",
    "\n",
    "print(\"Starting implementation Word2Vec\")\n",
    "# prepare transactions\n",
    "# same as for the baseline\n",
    "transactions_processed = transactions[\n",
    "    ['customer_id', 'age', 'article_id', 'prod_name', 'sales_channel_id', 'price', 'ordered']].copy()\n",
    "customer_encoder = preprocessing.LabelEncoder()\n",
    "article_encoder = preprocessing.LabelEncoder()\n",
    "transactions_processed['customer_id'] = customer_encoder.fit_transform(transactions_processed['customer_id'])\n",
    "transactions_processed['article_id'] = article_encoder.fit_transform(transactions_processed['article_id'])\n",
    "transactions_processed = transactions_processed.fillna(0)\n",
    "transactions_processed.isnull().values.any()\n",
    "transactions_processed = pd.get_dummies(transactions_processed, columns=['sales_channel_id'])\n",
    "\n",
    "# setup training set for word2vec\n",
    "train_frame = articles[\n",
    "    ['prod_name', 'product_type_name', 'product_group_name', 'graphical_appearance_name', 'department_name',\n",
    "     'index_name', 'index_group_name', 'section_name', 'garment_group_name']].drop_duplicates()\n",
    "train_frame = train_frame.apply(lambda x: ','.join(x.astype(str)), axis=1)\n",
    "train_frame = pd.DataFrame({'clean': train_frame})\n",
    "data = [row.split(',') for row in train_frame['clean']]\n",
    "\n",
    "# initialise and train model\n",
    "model = gensim.models.Word2Vec(min_count=1,\n",
    "                               vector_size=vec_size1,\n",
    "                               workers=7,\n",
    "                               window=3,\n",
    "                               sg=0)\n",
    "model.build_vocab(data)\n",
    "model.train(data, total_examples=model.corpus_count, epochs=30)\n",
    "\n",
    "# df to loop over\n",
    "p_names = pd.DataFrame(transactions_processed['prod_name']).drop_duplicates().reset_index(drop=True, inplace=False)\n",
    "# convert all names into vectors\n",
    "v = np.empty((0, vec_size1))\n",
    "for a in p_names.values:\n",
    "    vec = model.wv.__getitem__([a[0]])\n",
    "    v = np.append(v, vec, axis=0)\n",
    "\n",
    "# make a dataframe containing name and vector\n",
    "df = pd.DataFrame(v, columns=[f'f_{i}' for i in range(vec_size1)])\n",
    "df = pd.concat([p_names, df], axis=1)\n",
    "\n",
    "# merge dataframe with transactions\n",
    "transactions_processed = transactions_processed.merge(df, on='prod_name')\n",
    "\n",
    "# drop name and evaluate\n",
    "transactions_processed = transactions_processed.drop(['prod_name'], axis=1)\n",
    "evaluate(transactions_processed, vec_size=vec_size1)"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# detailed_desc Doc2Vec\n",
    "\n",
    "Evaluation may or may not work. Copying the code to a py file works more reliably.\n",
    "Reducing vector size may work (but give worse performance)."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "outputs": [],
   "source": [
    "vec_size2 = 50\n",
    "\n",
    "print(\"Starting implementation Doc2Vec\")\n",
    "\n",
    "# prepare transactions\n",
    "# same as for the baseline\n",
    "transactions_processed = transactions[\n",
    "    ['customer_id', 'age', 'article_id', 'prod_name', 'sales_channel_id', 'price', 'ordered', 'detail_desc']].copy()\n",
    "customer_encoder = preprocessing.LabelEncoder()\n",
    "article_encoder = preprocessing.LabelEncoder()\n",
    "transactions_processed['customer_id'] = customer_encoder.fit_transform(transactions_processed['customer_id'])\n",
    "transactions_processed['article_id'] = article_encoder.fit_transform(transactions_processed['article_id'])\n",
    "transactions_processed = transactions_processed.fillna(0)\n",
    "transactions_processed.isnull().values.any()\n",
    "transactions_processed = pd.get_dummies(transactions_processed, columns=['sales_channel_id'])\n",
    "\n",
    "# make training set\n",
    "train_frame = articles[['prod_name', 'detail_desc']].drop_duplicates()\n",
    "train_frame = train_frame.apply(lambda x: word_tokenize(\n",
    "    str(x['detail_desc']).lower().translate(str.maketrans(string.punctuation, ' ' * len(string.punctuation)))),\n",
    "                                axis=1)\n",
    "train_frame = pd.DataFrame({'clean': train_frame})\n",
    "data2 = [row for row in train_frame['clean']]\n",
    "\n",
    "# train model\n",
    "print(\"Starting training Doc2Vec\")\n",
    "model2 = gensim.models.Doc2Vec(min_count=1,\n",
    "                               vector_size=vec_size2,\n",
    "                               workers=7,\n",
    "                               window=3)\n",
    "\n",
    "data2 = [gensim.models.doc2vec.TaggedDocument(d, [i]) for i, d in enumerate(data2)]\n",
    "model2.build_vocab(data2)\n",
    "model2.train(data2, total_examples=model2.corpus_count, epochs=30)\n",
    "\n",
    "# df to loop over\n",
    "p_desc = pd.DataFrame(transactions_processed[['prod_name', 'detail_desc']]).drop_duplicates().reset_index(drop=True, inplace=False)\n",
    "\n",
    "# transform descriptions into vectors\n",
    "print(\"Starting vectorization\")\n",
    "v = np.empty((0, vec_size2))\n",
    "for i, a in np.ndenumerate(p_desc['detail_desc'].values):\n",
    "    desc = word_tokenize(str(a).lower().translate(str.maketrans(string.punctuation, ' ' * len(string.punctuation))))\n",
    "    vec = [model2.infer_vector(desc)]\n",
    "    v = np.append(v, vec, axis=0)\n",
    "    if i[0] % 1000 == 0:\n",
    "        print(f\"\\rProcessed {i[0]} / {p_desc.shape[0]} ({i[0]/p_desc.shape[0]*100:.2f}%) rows\", end=\"\")\n",
    "print()\n",
    "\n",
    "# make dataframe with desc + vector\n",
    "df = pd.DataFrame(v, columns=[f'f_{i}' for i in range(vec_size2)])\n",
    "df = pd.concat([p_desc, df], axis=1)\n",
    "\n",
    "# drop description to prevent duplicate columns\n",
    "transactions_processed = transactions_processed.drop(['detail_desc'], axis=1)\n",
    "\n",
    "# merge dataframe with transactions\n",
    "transactions_processed = transactions_processed.merge(df, on='prod_name')\n",
    "\n",
    "# drop product name and description\n",
    "transactions_processed = transactions_processed.drop(['prod_name', 'detail_desc'], axis=1)\n",
    "\n",
    "# evaluate\n",
    "evaluate(transactions_processed, vec_size2)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# W2V D2V Together\n",
    "\n",
    "Evaluation may or may not work. Copying the code to a py file works more reliably.\n",
    "Reducing the vector sizes in the previous models is almost a requirement, since 100 dimensional vectors will not be classified. A lenght of 25 for each vector seems like a good starting point.\n",
    "***Requires trained models from previous parts.***"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "outputs": [],
   "source": [
    "print(\"Starting implementation combination\")\n",
    "\n",
    "# prepare transactions\n",
    "# same as for the baseline\n",
    "transactions_processed = transactions[\n",
    "    ['customer_id', 'age', 'article_id', 'prod_name', 'sales_channel_id', 'price', 'ordered', 'detail_desc']].copy()\n",
    "customer_encoder = preprocessing.LabelEncoder()\n",
    "article_encoder = preprocessing.LabelEncoder()\n",
    "transactions_processed['customer_id'] = customer_encoder.fit_transform(transactions_processed['customer_id'])\n",
    "transactions_processed['article_id'] = article_encoder.fit_transform(transactions_processed['article_id'])\n",
    "transactions_processed = transactions_processed.fillna(0)\n",
    "transactions_processed.isnull().values.any()\n",
    "transactions_processed = pd.get_dummies(transactions_processed, columns=['sales_channel_id'])\n",
    "\n",
    "# df to loop over\n",
    "p_desc = pd.DataFrame(transactions_processed[['prod_name', 'detail_desc']]).drop_duplicates().reset_index(drop=True, inplace=False)\n",
    "\n",
    "# make a vector for each product\n",
    "print(\"Starting vectorization\")\n",
    "v = np.empty((0, vec_size1 + vec_size2))\n",
    "for i, row in enumerate(p_desc.values):\n",
    "    p_name = row[0]\n",
    "    desc = row[1]\n",
    "    desc = word_tokenize(\n",
    "        str(desc).lower().translate(str.maketrans(string.punctuation, ' ' * len(string.punctuation))))\n",
    "    desc_vec = model2.infer_vector(desc)\n",
    "    name_vec = model.wv[p_name]\n",
    "    vec = np.concatenate((name_vec, desc_vec), axis=None)\n",
    "    v = np.append(v, [vec], axis=0)\n",
    "    if i % 1000 == 0:\n",
    "        print(f\"\\rProcessed {i} / {p_desc.shape[0]} ({i/p_desc.shape[0]*100:.2f}%) rows\", end=\"\")\n",
    "print()\n",
    "\n",
    "# make a dataframe out of vectors and\n",
    "df = pd.DataFrame(v, columns=[f'f_{i}' for i in range(vec_size1 + vec_size2)])\n",
    "df = pd.concat([p_desc, df], axis=1)\n",
    "\n",
    "# drop column to prevent duplicate\n",
    "transactions_processed = transactions_processed.drop(['detail_desc'], axis=1)\n",
    "# merge dataframe with transactions\n",
    "transactions_processed = transactions_processed.merge(df, on='prod_name')\n",
    "# drop name and description\n",
    "transactions_processed = transactions_processed.drop(['prod_name', 'detail_desc'], axis=1)\n",
    "#evaluate\n",
    "evaluate(transactions_processed, vec_size1+vec_size2)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Recent popularity\n",
    "\n",
    "Somewhat slow"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Long loop\n",
      "Processed 1590000 / 1598761 rows\n",
      "Processed 3190000 / 3197522 (99.76%) rows\n",
      "         customer_id   age  article_id     price  ordered      t_dat  \\\n",
      "0                 14  22.0        4029  0.016932        1 2018-09-20   \n",
      "381716         48660  58.0       26263  0.033881        1 2018-09-20   \n",
      "1342250        67720  20.0         805  0.013542        0 2018-09-20   \n",
      "1342705        33681  27.0       11446  0.022017        1 2018-09-20   \n",
      "1342746        33681  27.0       13443  0.025407        1 2018-09-20   \n",
      "1342771        33681  27.0       11585  0.016932        1 2018-09-20   \n",
      "1861668        62179  21.0       22937  0.011847        1 2018-09-20   \n",
      "2718627        52818  26.0       17134  0.016932        0 2018-09-20   \n",
      "1343424        33779  50.0       21043  0.030492        1 2018-09-20   \n",
      "1343525        33779  50.0       25058  0.022864        1 2018-09-20   \n",
      "\n",
      "         sales_channel_id_1  sales_channel_id_2  rec_purchases  \n",
      "0                         0                   1              2  \n",
      "381716                    0                   1              2  \n",
      "1342250                   1                   0              0  \n",
      "1342705                   0                   1              0  \n",
      "1342746                   0                   1              0  \n",
      "1342771                   0                   1              0  \n",
      "1861668                   0                   1              0  \n",
      "2718627                   0                   1              0  \n",
      "1343424                   1                   0              0  \n",
      "1343525                   1                   0              0  \n",
      "Evaluating...\n",
      "              age     price  ordered  sales_channel_id_1  sales_channel_id_2  \\\n",
      "0       -1.001027 -0.553526        1                   0                   1   \n",
      "381716   1.575002  0.157003        1                   0                   1   \n",
      "1342250 -1.144140 -0.695632        0                   1                   0   \n",
      "1342705 -0.643246 -0.340368        1                   0                   1   \n",
      "1342746 -0.643246 -0.198262        1                   0                   1   \n",
      "...           ...       ...      ...                 ...                 ...   \n",
      "1906761 -0.857915 -0.553526        0                   0                   1   \n",
      "956111  -0.929471 -0.553526        1                   1                   0   \n",
      "2720285 -0.643246  0.849768        0                   0                   1   \n",
      "389108  -0.500133  0.157003        0                   1                   0   \n",
      "2081699  0.930995  1.222796        1                   0                   1   \n",
      "\n",
      "         rec_purchases  \n",
      "0            -0.147677  \n",
      "381716       -0.147677  \n",
      "1342250      -0.449337  \n",
      "1342705      -0.449337  \n",
      "1342746      -0.449337  \n",
      "...                ...  \n",
      "1906761      -0.449337  \n",
      "956111       -0.449337  \n",
      "2720285      -0.449337  \n",
      "389108       -0.449337  \n",
      "2081699      -0.147677  \n",
      "\n",
      "[3197522 rows x 6 columns]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.78      0.94      0.85    159830\n",
      "           1       0.92      0.74      0.82    159923\n",
      "\n",
      "    accuracy                           0.84    319753\n",
      "   macro avg       0.85      0.84      0.84    319753\n",
      "weighted avg       0.85      0.84      0.84    319753\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# prepare transactions\n",
    "# same as for the baseline\n",
    "transactions_processed = transactions[\n",
    "    ['customer_id', 'age', 'article_id', 'sales_channel_id', 'price', 'ordered', 't_dat']].copy()\n",
    "customer_encoder = preprocessing.LabelEncoder()\n",
    "article_encoder = preprocessing.LabelEncoder()\n",
    "transactions_processed['customer_id'] = customer_encoder.fit_transform(transactions_processed['customer_id'])\n",
    "transactions_processed['article_id'] = article_encoder.fit_transform(transactions_processed['article_id'])\n",
    "transactions_processed = transactions_processed.fillna(0)\n",
    "transactions_processed.isnull().values.any()\n",
    "transactions_processed = pd.get_dummies(transactions_processed, columns=['sales_channel_id'])\n",
    "\n",
    "# process dates\n",
    "transactions_processed['t_dat'] = pd.to_datetime(transactions_processed['t_dat'], format='%Y-%m-%d')\n",
    "transactions_processed = transactions_processed.sort_values(by=['t_dat'])\n",
    "\n",
    "# make dict of purchase dates\n",
    "purchase_dates = {}\n",
    "purchases = transactions_processed[transactions_processed['ordered'] == 1]  # only keep purchases\n",
    "purchases = purchases.sort_values(by=['t_dat'])\n",
    "purchases = purchases[['article_id', 't_dat']]\n",
    "for index, row in enumerate(purchases.values):\n",
    "    article_id = row[0]\n",
    "    date = row[1]\n",
    "    if article_id not in purchase_dates:\n",
    "        purchase_dates[article_id] = []\n",
    "    purchase_dates[article_id].append(date)\n",
    "    if index % 10000 == 0:\n",
    "        print(f\"\\rProcessed {index} / {purchases.shape[0]} rows\", end=\"\")\n",
    "print()\n",
    "\n",
    "# make a list of #recent purchases\n",
    "# takes a while\n",
    "rec_purchase_num = []\n",
    "for index, row in enumerate(transactions_processed.values):\n",
    "    article_id = row[2]\n",
    "    date = row[5]\n",
    "    if article_id in purchase_dates:\n",
    "        dates = purchase_dates[article_id]\n",
    "        dates = [d for d in dates if (date >= d >= date - pd.Timedelta(days=7))]\n",
    "        rec_purchase_num.append(len(dates) - row[4])\n",
    "    else:\n",
    "        rec_purchase_num.append(0)\n",
    "    if index % 10000 == 0:\n",
    "        print(f\"\\rProcessed {index} / {transactions_processed.shape[0]} ({index/transactions_processed.shape[0]*100:.2f}%) rows\", end=\"\")\n",
    "print()\n",
    "\n",
    "# add list to transactions\n",
    "transactions_processed['rec_purchases'] = rec_purchase_num\n",
    "print(transactions_processed.head(10))\n",
    "\n",
    "# drop dates\n",
    "transactions_processed = transactions_processed.drop(['t_dat'], axis=1)\n",
    "# evaluate\n",
    "evaluate(transactions_processed, vec_size=0, extra_vec=['rec_purchases'])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# W2V and Popularity\n",
    "\n",
    "Evaluation may or may not work. Copying the code to a py file works more reliably.\n",
    "Reducing vector size may work (but give worse performance).\n",
    "Requires trained models from previous parts."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "vec_size = 50\n",
    "\n",
    "print(\"Starting implementation combination w2v, pop\")\n",
    "\n",
    "# prepare transactions\n",
    "transactions_processed = transactions[\n",
    "    ['customer_id', 'age', 'article_id', 'sales_channel_id', 'price', 'ordered', 't_dat', 'prod_name']].copy()\n",
    "customer_encoder = preprocessing.LabelEncoder()\n",
    "article_encoder = preprocessing.LabelEncoder()\n",
    "transactions_processed['customer_id'] = customer_encoder.fit_transform(transactions_processed['customer_id'])\n",
    "transactions_processed['article_id'] = article_encoder.fit_transform(transactions_processed['article_id'])\n",
    "transactions_processed = transactions_processed.fillna(0)\n",
    "transactions_processed.isnull().values.any()\n",
    "transactions_processed = pd.get_dummies(transactions_processed, columns=['sales_channel_id'])\n",
    "\n",
    "transactions_processed['t_dat'] = pd.to_datetime(transactions_processed['t_dat'], format='%Y-%m-%d')\n",
    "transactions_processed = transactions_processed.sort_values(by=['t_dat'])\n",
    "\n",
    "# w2v part\n",
    "# generate vectors for all product names\n",
    "print(\"Starting vectorization\")\n",
    "p_names = pd.DataFrame(transactions_processed['prod_name']).drop_duplicates().reset_index(drop=True, inplace=False)\n",
    "v = np.empty((0, vec_size))\n",
    "for a in p_names.values:\n",
    "    vec = model.wv.__getitem__([a[0]])\n",
    "    v = np.append(v, vec, axis=0)\n",
    "df = pd.DataFrame(v, columns=[f'f_{i}' for i in range(vec_size)])\n",
    "df = pd.concat([p_names, df], axis=1)\n",
    "\n",
    "# pop part\n",
    "# make dict of purchase dates\n",
    "purchase_dates = {}\n",
    "purchases = transactions_processed[transactions_processed['ordered'] == 1]  # only keep purchases\n",
    "purchases = purchases.sort_values(by=['t_dat'])\n",
    "purchases = purchases[['article_id', 't_dat']]\n",
    "for index, row in enumerate(purchases.values):\n",
    "    article_id = row[0]\n",
    "    date = row[1]\n",
    "    if article_id not in purchase_dates:\n",
    "        purchase_dates[article_id] = []\n",
    "    purchase_dates[article_id].append(date)\n",
    "    if index % 10000 == 0:\n",
    "        print(f\"\\rProcessed {index} / {purchases.shape[0]} rows\", end=\"\")\n",
    "print()\n",
    "\n",
    "# make a list of #recent purchases\n",
    "rec_purchase_num = []\n",
    "for index, row in enumerate(transactions_processed.values):\n",
    "    article_id = row[2]\n",
    "    date = row[5]\n",
    "    if article_id in purchase_dates:\n",
    "        dates = purchase_dates[article_id]\n",
    "        dates = [d for d in dates if (date >= d >= date - pd.Timedelta(days=7))]\n",
    "        rec_purchase_num.append(len(dates) - row[4])\n",
    "    else:\n",
    "        rec_purchase_num.append(0)\n",
    "    if index % 10000 == 0:\n",
    "        print(f\"\\rProcessed {index} / {transactions_processed.shape[0]} ({index/transactions_processed.shape[0]*100:.2f}%) rows\", end=\"\")\n",
    "\n",
    "print()\n",
    "\n",
    "# add recent purchases to frame\n",
    "transactions_processed['rec_purchases'] = rec_purchase_num\n",
    "# merge with product name vectors\n",
    "transactions_processed = transactions_processed.merge(df, on='prod_name')\n",
    "# remove helper columns\n",
    "transactions_processed = transactions_processed.drop(['t_dat'], axis=1)\n",
    "transactions_processed = transactions_processed.drop(['prod_name'], axis=1)\n",
    "# evaluate\n",
    "evaluate(transactions_processed, vec_size=vec_size, extra_vec=['rec_purchases'])"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
