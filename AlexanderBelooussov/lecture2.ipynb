{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Lecture 2: Introduction to Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas\n",
    "import pandas as pd\n",
    "import random\n",
    "\n",
    "import sklearn.preprocessing\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn import preprocessing"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "articles = pd.read_csv('../data/articles.csv')\n",
    "customers = pd.read_csv('../data/customers.csv')\n",
    "sample_submisison = pd.read_csv('../data/sample_submission.csv')\n",
    "transactions = pd.read_csv('../data/transactions_train.csv')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## The H&M Dataset"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "articles.info()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "customers.info()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "transactions.info()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# X = transactions.merge(customers, how='inner', on='customer_id')\n",
    "# X = X.merge(articles, how='inner', on='article_id')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Creating Samples \n",
    "If you would rather work with samples instead of the whole dataset (while prototyping your code). You can use the code below:"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Adapted from: https://www.kaggle.com/code/paweljankiewicz/hm-create-dataset-samples\n",
    "# This extracts three sampled datasets, containing 0.1%, 1% and 5% of all users and their transactions, and the associated articles.\n",
    "for sample_repr, sample in [(\"01\", 0.001), (\"1\", 0.01), (\"5\", 0.05)]:\n",
    "    print(sample)\n",
    "    customers_sample = customers.sample(int(customers.shape[0]*sample), replace=False)\n",
    "    customers_sample_ids = set(customers_sample[\"customer_id\"])\n",
    "    transactions_sample = transactions[transactions[\"customer_id\"].isin(customers_sample_ids)]\n",
    "    articles_sample_ids = set(transactions_sample[\"article_id\"])\n",
    "    articles_sample = articles[articles[\"article_id\"].isin(articles_sample_ids)]\n",
    "    customers_sample.to_csv(f\"../data/customers_sample{sample_repr}.csv.gz\", index=False)\n",
    "    transactions_sample.to_csv(f\"../data/transactions_sample{sample_repr}.csv.gz\", index=False)\n",
    "    articles_sample.to_csv(f\"../data/articles_sample{sample_repr}.csv.gz\", index=False)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# articles_sample = pd.read_csv('../data/articles_sample01.csv.gz')\n",
    "# customers_sample = pd.read_csv('../data/customers_sample01.csv.gz')\n",
    "# transactions_sample = pd.read_csv('../data/transactions_sample01.csv.gz')\n",
    "articles_sample = pd.read_csv('../data/articles_sample5.csv.gz')\n",
    "customers_sample = pd.read_csv('../data/customers_sample5.csv.gz')\n",
    "transactions_sample = pd.read_csv('../data/transactions_sample5.csv.gz')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "customers_sample.info()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "transactions_sample.info()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## A Simplified Task: Binary Classification\n",
    "\n",
    "The task of predicting which 12 items users are most likely to buy in the next week is difficult to translate to a traditional classification machine learning setting. \n",
    "To obtain the 12 items a user is most likely to buy, we need to make predictions for all items (or the ones selected by a baseline) and select the 12 that have the highest predicted scores.\n",
    "\n",
    "In this assignment, we'll consider a simplified task: Predict whether a user ordered a single item or not, based on the features of the user and the item. \n",
    "We provide a baseline logistic regression model below, but haven't done much feature preprocessing or engineering!\n",
    "Initially, it is always best to focus your efforts on getting your features in the right shape and setting up the right validation scheme and baselines.\n",
    "Once you are sure that your features add value and your validation scheme is correct, then you typically move on to trying more elaborate models."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Creating the Dataset"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# If you'd rather use a sample. Uncomment the following code:\n",
    "transactions = transactions_sample\n",
    "customers = customers_sample\n",
    "articles = articles_sample"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "transactions['ordered'] = 1"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "The problem setting is an example of a \"PU learning\" problem, i.e. only positives are labeled, everything else is unlabeled (and can be either positive or negative). \n",
    "Of course, we cannot train a classifier with just positive samples: The classifier will just learn that everything is positive.\n",
    "Therefore, we need to manually generate negative samples.\n",
    "\n",
    "Below, we use a simple random negative sampling strategy.\n",
    "We want to create a balanced dataset, meaning that we have just as many positives as negatives.\n",
    "This makes sure that the classifier will not benefit from predicting the positive/negative class more often than the other.\n",
    "Realistically, the amount of positive samples is of course many times smaller than the amount of unlabeled, possibly negative instances.\n",
    "\n",
    "\n",
    "If you want to try your hand at a more complex negative sampling strategy, you may want to check out this blog as a starting point: https://medium.com/mlearning-ai/overview-negative-sampling-on-recommendation-systems-230a051c6cd7.\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "transactions.head()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# What's happening here? \n",
    "# We're creating negative samples. I.e. we're creating transactions that didn't actually occur.\n",
    "# First, we need to know which interactions did occur:\n",
    "positive_pairs = list(map(tuple, transactions[['customer_id', 'article_id']].drop_duplicates().values))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Then we need to know what every synthetic transaction should contain: a date, a customer_id, an article_id, price, sales_channel_id. We will set ordered = 0, as these transactions didn't really occur.\n",
    "transactions.head()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Extract real values\n",
    "real_dates = transactions[\"t_dat\"].unique()\n",
    "real_customers = transactions[\"customer_id\"].unique()\n",
    "real_articles = transactions[\"article_id\"].unique()\n",
    "real_channels = transactions[\"sales_channel_id\"].unique()\n",
    "article_and_price = transactions[[\"article_id\",\"price\"]].drop_duplicates(\"article_id\").set_index(\"article_id\").squeeze()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# How many negatives do we need to sample?\n",
    "num_neg_pos = transactions.shape[0]\n",
    "print(num_neg_pos)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Sampling negatives by selecting random users, articles, dates and sales channel:\n",
    "# Note: This is quite naive. Some articles may not even have been available at the date we are sampling.\n",
    "random.seed(42)\n",
    "\n",
    "# Afterwards, we need to remove potential duplicates, so we'll sample too many.\n",
    "num_neg_samples = int(num_neg_pos * 1.1)\n",
    "\n",
    "# Sample each of the independent attributes.\n",
    "neg_dates = np.random.choice(real_dates, size=num_neg_samples)\n",
    "neg_articles = np.random.choice(real_articles, size=num_neg_samples)\n",
    "neg_customers = np.random.choice(real_customers, size=num_neg_samples)\n",
    "neg_channels = np.random.choice(real_channels, size=num_neg_samples)\n",
    "ordered = np.array([0] * num_neg_samples)\n",
    "# Assign to every article a real price.\n",
    "neg_prices = article_and_price[neg_articles].values"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "neg_transactions = pd.DataFrame([neg_dates, neg_customers, neg_articles, neg_prices, neg_channels, ordered], index=transactions.columns).T"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Result:\n",
    "neg_transactions.head()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "neg_transactions.shape"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Remove random negative samples that actually coincide with positives\n",
    "df = neg_transactions[\n",
    "    ~neg_transactions.set_index([\"customer_id\", \"article_id\"]).index.isin(positive_pairs)\n",
    "]\n",
    "\n",
    "# Remove any excess\n",
    "chosen_neg_transactions = df.sample(num_neg_pos)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Concat the negative samples to the positive samples:\n",
    "transactions = pd.concat([transactions, chosen_neg_transactions])\n",
    "transactions = transactions.merge(customers, how=\"inner\", on='customer_id')\n",
    "transactions = transactions.merge(articles, how=\"inner\", on='article_id')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "transactions.info()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Basic Preprocessing\n",
    "Some very basic preprocessing."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# I'm dropping a lot of columns, use them in your engineering tasks!\n",
    "transactions_processed = transactions[['customer_id', 'age', 'article_id', 'sales_channel_id', 'price', 'ordered']].copy()\n",
    "transactions_processed.head()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Does it make sense to label encode?\n",
    "# Label encoding the customer and article IDs:\n",
    "customer_encoder = preprocessing.LabelEncoder()\n",
    "customer_encoder = customer_encoder.fit(transactions_processed['customer_id'])\n",
    "article_encoder = preprocessing.LabelEncoder()\n",
    "article_encoder = article_encoder.fit(transactions_processed['article_id'])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "transactions_processed['customer_id'] = customer_encoder.transform(transactions_processed['customer_id'])\n",
    "transactions_processed['article_id'] = article_encoder.transform(transactions_processed['article_id'])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# If you want to go back to the original encoding:\n",
    "customer_encoder.inverse_transform([2])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "transactions_processed.head()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Can you come up with a NaN strategy that makes sense for each column in the dataset?\n",
    "# Imputing all NaN values with zeros:\n",
    "transactions_processed = transactions_processed.fillna(0)\n",
    "transactions_processed.isnull().values.any()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Does it make sense to one-hot encode?\n",
    "# One-hot-encoding sales_channel_id:\n",
    "transactions_processed = pd.get_dummies(transactions_processed, columns=['sales_channel_id'])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "transactions_processed.head()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Creating a Train / Test Split:\n",
    "X_train, X_test, y_train, y_test = train_test_split(transactions_processed.drop('ordered', axis=1), transactions_processed['ordered'], test_size=0.10, random_state=42)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "X_train.head()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "y_train.head()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Baseline Model"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Will take a few minutes to run, if you're using the whole dataset:\n",
    "baseline = LogisticRegression(random_state=42, n_jobs=6)\n",
    "baseline = baseline.fit(X_train, y_train)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "baseline.predict_proba(X_test)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "y_test"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Mean Accuracy:\n",
    "baseline.score(X_test, y_test)\n",
    "# As you can seen, the accuracy is ~0.51. In other words, the classifier predicts correctly 51% of the time whether a customer did or din't buy an item.\n",
    "# Can you improve this baseline logistic regression model by doing better preprocessing and generating new features?\n",
    "# Also think about my steps! Did it make sense to include the article and customer ids? (And things like that)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Classification Metrics:\n",
    "predictions = baseline.predict(X_test)\n",
    "print(classification_report(y_test, predictions))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "predictions"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Assignment: Feature engineering\n",
    "**TODO:** \n",
    "- In groups (of 2-3 students), think about (a few) features that can be engineered (preprocess and generate new features). Divide the work!\n",
    "- Do these engineered features improve the baseline model?\n",
    "- Add your thoughts & results to a slide deck for discussion next week (again, 1 slide per person).\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# prod_name Word2Vec\n",
    "Evaluation may or may not work. Copying the code to a py file works more reliably.\n",
    "Reducing vector size may work (but give worse performance).\n",
    "\n",
    "\n",
    "The Following blocks of code were originally run in a .py file (for stability/easier debugging) and copied over to the notebook."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import gensim\n",
    "import string\n",
    "from nltk import word_tokenize\n",
    "def evaluate(frame, vec_size=50, extra_vec=None):\n",
    "    \"\"\"\n",
    "    Evaluates frame using LogisticRegression\n",
    "    \"\"\"\n",
    "    print(\"Evaluating...\")\n",
    "    frame = frame.copy()\n",
    "    seed = 42\n",
    "    frame = frame.drop(['customer_id', 'article_id'], axis=1)\n",
    "    scaler = preprocessing.StandardScaler()\n",
    "    # scaler = preprocessing.MinMaxScaler()\n",
    "    frame[['age', 'price']] = scaler.fit_transform(frame[['age', 'price']])\n",
    "    if vec_size > 0:\n",
    "        vec_idx = range(vec_size)\n",
    "        frame[vec_idx] = scaler.fit_transform(frame[vec_idx])\n",
    "    if extra_vec is not None:\n",
    "        frame[extra_vec] = scaler.fit_transform(frame[extra_vec])\n",
    "    print(frame)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(frame.drop('ordered', axis=1), frame['ordered'], test_size=0.10,\n",
    "                                                        random_state=seed)\n",
    "    # Will take a few minutes to run, if you're using the whole dataset:\n",
    "    better = LogisticRegression(random_state=seed, n_jobs=7, verbose=False, max_iter=1000)\n",
    "    better = better.fit(X_train, y_train)\n",
    "    better.predict_proba(X_test)\n",
    "    better.score(X_test, y_test)\n",
    "    better_predictions = better.predict(X_test)\n",
    "    print(classification_report(y_test, better_predictions))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "vec_size1 = 50\n",
    "\n",
    "print(\"Starting implementation Word2Vec\")\n",
    "# prepare transactions\n",
    "# same as for the baseline\n",
    "transactions_processed = transactions[\n",
    "    ['customer_id', 'age', 'article_id', 'prod_name', 'sales_channel_id', 'price', 'ordered']].copy()\n",
    "customer_encoder = preprocessing.LabelEncoder()\n",
    "article_encoder = preprocessing.LabelEncoder()\n",
    "transactions_processed['customer_id'] = customer_encoder.fit_transform(transactions_processed['customer_id'])\n",
    "transactions_processed['article_id'] = article_encoder.fit_transform(transactions_processed['article_id'])\n",
    "transactions_processed = transactions_processed.fillna(0)\n",
    "transactions_processed.isnull().values.any()\n",
    "transactions_processed = pd.get_dummies(transactions_processed, columns=['sales_channel_id'])\n",
    "\n",
    "# setup training set for word2vec\n",
    "train_frame = articles[\n",
    "    ['prod_name', 'product_type_name', 'product_group_name', 'graphical_appearance_name', 'department_name',\n",
    "     'index_name', 'index_group_name', 'section_name', 'garment_group_name']].drop_duplicates()\n",
    "train_frame = train_frame.apply(lambda x: ','.join(x.astype(str)), axis=1)\n",
    "train_frame = pd.DataFrame({'clean': train_frame})\n",
    "data = [row.split(',') for row in train_frame['clean']]\n",
    "\n",
    "# initialise and train model\n",
    "model = gensim.models.Word2Vec(min_count=1,\n",
    "                               vector_size=vec_size1,\n",
    "                               workers=7,\n",
    "                               window=3,\n",
    "                               sg=0)\n",
    "model.build_vocab(data)\n",
    "model.train(data, total_examples=model.corpus_count, epochs=30)\n",
    "\n",
    "# df to loop over\n",
    "p_names = pd.DataFrame(transactions_processed['prod_name']).drop_duplicates().reset_index(drop=True, inplace=False)\n",
    "# convert all names into vectors\n",
    "v = np.empty((0, vec_size1))\n",
    "for a in p_names.values:\n",
    "    vec = model.wv.__getitem__([a[0]])\n",
    "    v = np.append(v, vec, axis=0)\n",
    "\n",
    "# make a dataframe containing name and vector\n",
    "df = pd.DataFrame(v, columns=[f'f_{i}' for i in range(vec_size1)])\n",
    "df = pd.concat([p_names, df], axis=1)\n",
    "\n",
    "# merge dataframe with transactions\n",
    "transactions_processed = transactions_processed.merge(df, on='prod_name')\n",
    "\n",
    "# drop name and evaluate\n",
    "transactions_processed = transactions_processed.drop(['prod_name'], axis=1)\n",
    "evaluate(transactions_processed, vec_size=vec_size1)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# detailed_desc Doc2Vec\n",
    "\n",
    "Evaluation may or may not work. Copying the code to a py file works more reliably.\n",
    "Reducing vector size may work (but give worse performance)."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "vec_size2 = 50\n",
    "\n",
    "print(\"Starting implementation Doc2Vec\")\n",
    "\n",
    "# prepare transactions\n",
    "# same as for the baseline\n",
    "transactions_processed = transactions[\n",
    "    ['customer_id', 'age', 'article_id', 'prod_name', 'sales_channel_id', 'price', 'ordered', 'detail_desc']].copy()\n",
    "customer_encoder = preprocessing.LabelEncoder()\n",
    "article_encoder = preprocessing.LabelEncoder()\n",
    "transactions_processed['customer_id'] = customer_encoder.fit_transform(transactions_processed['customer_id'])\n",
    "transactions_processed['article_id'] = article_encoder.fit_transform(transactions_processed['article_id'])\n",
    "transactions_processed = transactions_processed.fillna(0)\n",
    "transactions_processed.isnull().values.any()\n",
    "transactions_processed = pd.get_dummies(transactions_processed, columns=['sales_channel_id'])\n",
    "\n",
    "# make training set\n",
    "train_frame = articles[['prod_name', 'detail_desc']].drop_duplicates()\n",
    "train_frame = train_frame.apply(lambda x: word_tokenize(\n",
    "    str(x['detail_desc']).lower().translate(str.maketrans(string.punctuation, ' ' * len(string.punctuation)))),\n",
    "                                axis=1)\n",
    "train_frame = pd.DataFrame({'clean': train_frame})\n",
    "data2 = [row for row in train_frame['clean']]\n",
    "\n",
    "# train model\n",
    "print(\"Starting training Doc2Vec\")\n",
    "model2 = gensim.models.Doc2Vec(min_count=1,\n",
    "                               vector_size=vec_size2,\n",
    "                               workers=7,\n",
    "                               window=3)\n",
    "\n",
    "data2 = [gensim.models.doc2vec.TaggedDocument(d, [i]) for i, d in enumerate(data2)]\n",
    "model2.build_vocab(data2)\n",
    "model2.train(data2, total_examples=model2.corpus_count, epochs=30)\n",
    "\n",
    "# df to loop over\n",
    "p_desc = pd.DataFrame(transactions_processed[['prod_name', 'detail_desc']]).drop_duplicates().reset_index(drop=True, inplace=False)\n",
    "\n",
    "# transform descriptions into vectors\n",
    "print(\"Starting vectorization\")\n",
    "v = np.empty((0, vec_size2))\n",
    "for i, a in np.ndenumerate(p_desc['detail_desc'].values):\n",
    "    desc = word_tokenize(str(a).lower().translate(str.maketrans(string.punctuation, ' ' * len(string.punctuation))))\n",
    "    vec = [model2.infer_vector(desc)]\n",
    "    v = np.append(v, vec, axis=0)\n",
    "    if i[0] % 1000 == 0:\n",
    "        print(f\"\\rProcessed {i[0]} / {p_desc.shape[0]} ({i[0]/p_desc.shape[0]*100:.2f}%) rows\", end=\"\")\n",
    "print()\n",
    "\n",
    "# make dataframe with desc + vector\n",
    "df = pd.DataFrame(v, columns=[f'f_{i}' for i in range(vec_size2)])\n",
    "df = pd.concat([p_desc, df], axis=1)\n",
    "\n",
    "# drop description to prevent duplicate columns\n",
    "transactions_processed = transactions_processed.drop(['detail_desc'], axis=1)\n",
    "\n",
    "# merge dataframe with transactions\n",
    "transactions_processed = transactions_processed.merge(df, on='prod_name')\n",
    "\n",
    "# drop product name and description\n",
    "transactions_processed = transactions_processed.drop(['prod_name', 'detail_desc'], axis=1)\n",
    "\n",
    "# evaluate\n",
    "evaluate(transactions_processed, vec_size2)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# W2V D2V Together\n",
    "\n",
    "Evaluation may or may not work. Copying the code to a py file works more reliably.\n",
    "Reducing the vector sizes in the previous models is almost a requirement, since 100 dimensional vectors will not be classified. A lenght of 25 for each vector seems like a good starting point.\n",
    "***Requires trained models from previous parts.***"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print(\"Starting implementation combination\")\n",
    "\n",
    "# prepare transactions\n",
    "# same as for the baseline\n",
    "transactions_processed = transactions[\n",
    "    ['customer_id', 'age', 'article_id', 'prod_name', 'sales_channel_id', 'price', 'ordered', 'detail_desc']].copy()\n",
    "customer_encoder = preprocessing.LabelEncoder()\n",
    "article_encoder = preprocessing.LabelEncoder()\n",
    "transactions_processed['customer_id'] = customer_encoder.fit_transform(transactions_processed['customer_id'])\n",
    "transactions_processed['article_id'] = article_encoder.fit_transform(transactions_processed['article_id'])\n",
    "transactions_processed = transactions_processed.fillna(0)\n",
    "transactions_processed.isnull().values.any()\n",
    "transactions_processed = pd.get_dummies(transactions_processed, columns=['sales_channel_id'])\n",
    "\n",
    "# df to loop over\n",
    "p_desc = pd.DataFrame(transactions_processed[['prod_name', 'detail_desc']]).drop_duplicates().reset_index(drop=True, inplace=False)\n",
    "\n",
    "# make a vector for each product\n",
    "print(\"Starting vectorization\")\n",
    "v = np.empty((0, vec_size1 + vec_size2))\n",
    "for i, row in enumerate(p_desc.values):\n",
    "    p_name = row[0]\n",
    "    desc = row[1]\n",
    "    desc = word_tokenize(\n",
    "        str(desc).lower().translate(str.maketrans(string.punctuation, ' ' * len(string.punctuation))))\n",
    "    desc_vec = model2.infer_vector(desc)\n",
    "    name_vec = model.wv[p_name]\n",
    "    vec = np.concatenate((name_vec, desc_vec), axis=None)\n",
    "    v = np.append(v, [vec], axis=0)\n",
    "    if i % 1000 == 0:\n",
    "        print(f\"\\rProcessed {i} / {p_desc.shape[0]} ({i/p_desc.shape[0]*100:.2f}%) rows\", end=\"\")\n",
    "print()\n",
    "\n",
    "# make a dataframe out of vectors and\n",
    "df = pd.DataFrame(v, columns=[f'f_{i}' for i in range(vec_size1 + vec_size2)])\n",
    "df = pd.concat([p_desc, df], axis=1)\n",
    "\n",
    "# drop column to prevent duplicate\n",
    "transactions_processed = transactions_processed.drop(['detail_desc'], axis=1)\n",
    "# merge dataframe with transactions\n",
    "transactions_processed = transactions_processed.merge(df, on='prod_name')\n",
    "# drop name and description\n",
    "transactions_processed = transactions_processed.drop(['prod_name', 'detail_desc'], axis=1)\n",
    "#evaluate\n",
    "evaluate(transactions_processed, vec_size1+vec_size2)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Recent popularity\n",
    "\n",
    "Somewhat slow"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# prepare transactions\n",
    "# same as for the baseline\n",
    "transactions_processed = transactions[\n",
    "    ['customer_id', 'age', 'article_id', 'sales_channel_id', 'price', 'ordered', 't_dat']].copy()\n",
    "customer_encoder = preprocessing.LabelEncoder()\n",
    "article_encoder = preprocessing.LabelEncoder()\n",
    "transactions_processed['customer_id'] = customer_encoder.fit_transform(transactions_processed['customer_id'])\n",
    "transactions_processed['article_id'] = article_encoder.fit_transform(transactions_processed['article_id'])\n",
    "transactions_processed = transactions_processed.fillna(0)\n",
    "transactions_processed.isnull().values.any()\n",
    "transactions_processed = pd.get_dummies(transactions_processed, columns=['sales_channel_id'])\n",
    "\n",
    "# process dates\n",
    "transactions_processed['t_dat'] = pd.to_datetime(transactions_processed['t_dat'], format='%Y-%m-%d')\n",
    "transactions_processed = transactions_processed.sort_values(by=['t_dat'])\n",
    "\n",
    "# make dict of purchase dates\n",
    "purchase_dates = {}\n",
    "purchases = transactions_processed[transactions_processed['ordered'] == 1]  # only keep purchases\n",
    "purchases = purchases.sort_values(by=['t_dat'])\n",
    "purchases = purchases[['article_id', 't_dat']]\n",
    "for index, row in enumerate(purchases.values):\n",
    "    article_id = row[0]\n",
    "    date = row[1]\n",
    "    if article_id not in purchase_dates:\n",
    "        purchase_dates[article_id] = []\n",
    "    purchase_dates[article_id].append(date)\n",
    "    if index % 10000 == 0:\n",
    "        print(f\"\\rProcessed {index} / {purchases.shape[0]} rows\", end=\"\")\n",
    "print()\n",
    "\n",
    "# make a list of #recent purchases\n",
    "# takes a while\n",
    "rec_purchase_num = []\n",
    "for index, row in enumerate(transactions_processed.values):\n",
    "    article_id = row[2]\n",
    "    date = row[5]\n",
    "    if article_id in purchase_dates:\n",
    "        dates = purchase_dates[article_id]\n",
    "        dates = [d for d in dates if (date >= d >= date - pd.Timedelta(days=7))]\n",
    "        rec_purchase_num.append(len(dates) - row[4])\n",
    "    else:\n",
    "        rec_purchase_num.append(0)\n",
    "    if index % 10000 == 0:\n",
    "        print(f\"\\rProcessed {index} / {transactions_processed.shape[0]} ({index/transactions_processed.shape[0]*100:.2f}%) rows\", end=\"\")\n",
    "print()\n",
    "\n",
    "# add list to transactions\n",
    "transactions_processed['rec_purchases'] = rec_purchase_num\n",
    "print(transactions_processed.head(10))\n",
    "\n",
    "# drop dates\n",
    "transactions_processed = transactions_processed.drop(['t_dat'], axis=1)\n",
    "# evaluate\n",
    "evaluate(transactions_processed, vec_size=0, extra_vec=['rec_purchases'])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# W2V and Popularity\n",
    "\n",
    "Evaluation may or may not work. Copying the code to a py file works more reliably.\n",
    "Reducing vector size may work (but give worse performance).\n",
    "Requires trained models from previous parts."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "vec_size = 50\n",
    "\n",
    "print(\"Starting implementation combination w2v, pop\")\n",
    "\n",
    "# prepare transactions\n",
    "transactions_processed = transactions[\n",
    "    ['customer_id', 'age', 'article_id', 'sales_channel_id', 'price', 'ordered', 't_dat', 'prod_name']].copy()\n",
    "customer_encoder = preprocessing.LabelEncoder()\n",
    "article_encoder = preprocessing.LabelEncoder()\n",
    "transactions_processed['customer_id'] = customer_encoder.fit_transform(transactions_processed['customer_id'])\n",
    "transactions_processed['article_id'] = article_encoder.fit_transform(transactions_processed['article_id'])\n",
    "transactions_processed = transactions_processed.fillna(0)\n",
    "transactions_processed.isnull().values.any()\n",
    "transactions_processed = pd.get_dummies(transactions_processed, columns=['sales_channel_id'])\n",
    "\n",
    "transactions_processed['t_dat'] = pd.to_datetime(transactions_processed['t_dat'], format='%Y-%m-%d')\n",
    "transactions_processed = transactions_processed.sort_values(by=['t_dat'])\n",
    "\n",
    "# w2v part\n",
    "# generate vectors for all product names\n",
    "print(\"Starting vectorization\")\n",
    "p_names = pd.DataFrame(transactions_processed['prod_name']).drop_duplicates().reset_index(drop=True, inplace=False)\n",
    "v = np.empty((0, vec_size))\n",
    "for a in p_names.values:\n",
    "    vec = model.wv.__getitem__([a[0]])\n",
    "    v = np.append(v, vec, axis=0)\n",
    "df = pd.DataFrame(v, columns=[f'f_{i}' for i in range(vec_size)])\n",
    "df = pd.concat([p_names, df], axis=1)\n",
    "\n",
    "# pop part\n",
    "# make dict of purchase dates\n",
    "purchase_dates = {}\n",
    "purchases = transactions_processed[transactions_processed['ordered'] == 1]  # only keep purchases\n",
    "purchases = purchases.sort_values(by=['t_dat'])\n",
    "purchases = purchases[['article_id', 't_dat']]\n",
    "for index, row in enumerate(purchases.values):\n",
    "    article_id = row[0]\n",
    "    date = row[1]\n",
    "    if article_id not in purchase_dates:\n",
    "        purchase_dates[article_id] = []\n",
    "    purchase_dates[article_id].append(date)\n",
    "    if index % 10000 == 0:\n",
    "        print(f\"\\rProcessed {index} / {purchases.shape[0]} rows\", end=\"\")\n",
    "print()\n",
    "\n",
    "# make a list of #recent purchases\n",
    "rec_purchase_num = []\n",
    "for index, row in enumerate(transactions_processed.values):\n",
    "    article_id = row[2]\n",
    "    date = row[5]\n",
    "    if article_id in purchase_dates:\n",
    "        dates = purchase_dates[article_id]\n",
    "        dates = [d for d in dates if (date >= d >= date - pd.Timedelta(days=7))]\n",
    "        rec_purchase_num.append(len(dates) - row[4])\n",
    "    else:\n",
    "        rec_purchase_num.append(0)\n",
    "    if index % 10000 == 0:\n",
    "        print(f\"\\rProcessed {index} / {transactions_processed.shape[0]} ({index/transactions_processed.shape[0]*100:.2f}%) rows\", end=\"\")\n",
    "\n",
    "print()\n",
    "\n",
    "# add recent purchases to frame\n",
    "transactions_processed['rec_purchases'] = rec_purchase_num\n",
    "# merge with product name vectors\n",
    "transactions_processed = transactions_processed.merge(df, on='prod_name')\n",
    "# remove helper columns\n",
    "transactions_processed = transactions_processed.drop(['t_dat'], axis=1)\n",
    "transactions_processed = transactions_processed.drop(['prod_name'], axis=1)\n",
    "# evaluate\n",
    "evaluate(transactions_processed, vec_size=vec_size, extra_vec=['rec_purchases'])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "outputs": [],
   "source": [
    "import gensim\n",
    "import string\n",
    "from nltk import word_tokenize\n",
    "def evaluate(frame, vec_size=50, extra_vec=None):\n",
    "    \"\"\"\n",
    "    Evaluates frame using LogisticRegression\n",
    "    \"\"\"\n",
    "    print(\"Evaluating...\")\n",
    "    frame = frame.copy()\n",
    "    seed = 42\n",
    "    frame = frame.drop(['customer_id', 'article_id'], axis=1)\n",
    "    scaler = preprocessing.StandardScaler()\n",
    "    # scaler = preprocessing.MinMaxScaler()\n",
    "    frame[['age', 'price']] = scaler.fit_transform(frame[['age', 'price']])\n",
    "    if vec_size > 0:\n",
    "        vec_idx = range(vec_size)\n",
    "        frame[vec_idx] = scaler.fit_transform(frame[vec_idx])\n",
    "    if extra_vec is not None:\n",
    "        frame[extra_vec] = scaler.fit_transform(frame[extra_vec])\n",
    "    print(frame)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(frame.drop('ordered', axis=1), frame['ordered'], test_size=0.10,\n",
    "                                                        random_state=seed)\n",
    "    # Will take a few minutes to run, if you're using the whole dataset:\n",
    "    better = LogisticRegression(random_state=seed, n_jobs=7, verbose=False, max_iter=1000)\n",
    "    better = better.fit(X_train, y_train)\n",
    "    better.predict_proba(X_test)\n",
    "    better.score(X_test, y_test)\n",
    "    better_predictions = better.predict(X_test)\n",
    "    print(classification_report(y_test, better_predictions))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "vec_size1 = 50\n",
    "\n",
    "print(\"Starting implementation Word2Vec\")\n",
    "# prepare transactions\n",
    "# same as for the baseline\n",
    "transactions_processed = transactions[\n",
    "    ['customer_id', 'age', 'article_id', 'prod_name', 'sales_channel_id', 'price', 'ordered']].copy()\n",
    "customer_encoder = preprocessing.LabelEncoder()\n",
    "article_encoder = preprocessing.LabelEncoder()\n",
    "transactions_processed['customer_id'] = customer_encoder.fit_transform(transactions_processed['customer_id'])\n",
    "transactions_processed['article_id'] = article_encoder.fit_transform(transactions_processed['article_id'])\n",
    "transactions_processed = transactions_processed.fillna(0)\n",
    "transactions_processed.isnull().values.any()\n",
    "transactions_processed = pd.get_dummies(transactions_processed, columns=['sales_channel_id'])\n",
    "\n",
    "# setup training set for word2vec\n",
    "train_frame = articles[\n",
    "    ['prod_name', 'product_type_name', 'product_group_name', 'graphical_appearance_name', 'department_name',\n",
    "     'index_name', 'index_group_name', 'section_name', 'garment_group_name']].drop_duplicates()\n",
    "train_frame = train_frame.apply(lambda x: ','.join(x.astype(str)), axis=1)\n",
    "train_frame = pd.DataFrame({'clean': train_frame})\n",
    "data = [row.split(',') for row in train_frame['clean']]\n",
    "\n",
    "# initialise and train model\n",
    "model = gensim.models.Word2Vec(min_count=1,\n",
    "                               vector_size=vec_size1,\n",
    "                               workers=7,\n",
    "                               window=3,\n",
    "                               sg=0)\n",
    "model.build_vocab(data)\n",
    "model.train(data, total_examples=model.corpus_count, epochs=30)\n",
    "\n",
    "# df to loop over\n",
    "p_names = pd.DataFrame(transactions_processed['prod_name']).drop_duplicates().reset_index(drop=True, inplace=False)\n",
    "# convert all names into vectors\n",
    "v = np.empty((0, vec_size1))\n",
    "for a in p_names.values:\n",
    "    vec = model.wv.__getitem__([a[0]])\n",
    "    v = np.append(v, vec, axis=0)\n",
    "\n",
    "# make a dataframe containing name and vector\n",
    "df = pd.DataFrame(v, columns=[f'f_{i}' for i in range(vec_size1)])\n",
    "df = pd.concat([p_names, df], axis=1)\n",
    "\n",
    "# merge dataframe with transactions\n",
    "transactions_processed = transactions_processed.merge(df, on='prod_name')\n",
    "\n",
    "# drop name and evaluate\n",
    "transactions_processed = transactions_processed.drop(['prod_name'], axis=1)\n",
    "evaluate(transactions_processed, vec_size=vec_size1)"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# detailed_desc Doc2Vec\n",
    "\n",
    "Evaluation may or may not work. Copying the code to a py file works more reliably.\n",
    "Reducing vector size may work (but give worse performance)."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "outputs": [],
   "source": [
    "vec_size2 = 50\n",
    "\n",
    "print(\"Starting implementation Doc2Vec\")\n",
    "\n",
    "# prepare transactions\n",
    "# same as for the baseline\n",
    "transactions_processed = transactions[\n",
    "    ['customer_id', 'age', 'article_id', 'prod_name', 'sales_channel_id', 'price', 'ordered', 'detail_desc']].copy()\n",
    "customer_encoder = preprocessing.LabelEncoder()\n",
    "article_encoder = preprocessing.LabelEncoder()\n",
    "transactions_processed['customer_id'] = customer_encoder.fit_transform(transactions_processed['customer_id'])\n",
    "transactions_processed['article_id'] = article_encoder.fit_transform(transactions_processed['article_id'])\n",
    "transactions_processed = transactions_processed.fillna(0)\n",
    "transactions_processed.isnull().values.any()\n",
    "transactions_processed = pd.get_dummies(transactions_processed, columns=['sales_channel_id'])\n",
    "\n",
    "# make training set\n",
    "train_frame = articles[['prod_name', 'detail_desc']].drop_duplicates()\n",
    "train_frame = train_frame.apply(lambda x: word_tokenize(\n",
    "    str(x['detail_desc']).lower().translate(str.maketrans(string.punctuation, ' ' * len(string.punctuation)))),\n",
    "                                axis=1)\n",
    "train_frame = pd.DataFrame({'clean': train_frame})\n",
    "data2 = [row for row in train_frame['clean']]\n",
    "\n",
    "# train model\n",
    "print(\"Starting training Doc2Vec\")\n",
    "model2 = gensim.models.Doc2Vec(min_count=1,\n",
    "                               vector_size=vec_size2,\n",
    "                               workers=7,\n",
    "                               window=3)\n",
    "\n",
    "data2 = [gensim.models.doc2vec.TaggedDocument(d, [i]) for i, d in enumerate(data2)]\n",
    "model2.build_vocab(data2)\n",
    "model2.train(data2, total_examples=model2.corpus_count, epochs=30)\n",
    "\n",
    "# df to loop over\n",
    "p_desc = pd.DataFrame(transactions_processed[['prod_name', 'detail_desc']]).drop_duplicates().reset_index(drop=True, inplace=False)\n",
    "\n",
    "# transform descriptions into vectors\n",
    "print(\"Starting vectorization\")\n",
    "v = np.empty((0, vec_size2))\n",
    "for i, a in np.ndenumerate(p_desc['detail_desc'].values):\n",
    "    desc = word_tokenize(str(a).lower().translate(str.maketrans(string.punctuation, ' ' * len(string.punctuation))))\n",
    "    vec = [model2.infer_vector(desc)]\n",
    "    v = np.append(v, vec, axis=0)\n",
    "    if i[0] % 1000 == 0:\n",
    "        print(f\"\\rProcessed {i[0]} / {p_desc.shape[0]} ({i[0]/p_desc.shape[0]*100:.2f}%) rows\", end=\"\")\n",
    "print()\n",
    "\n",
    "# make dataframe with desc + vector\n",
    "df = pd.DataFrame(v, columns=[f'f_{i}' for i in range(vec_size2)])\n",
    "df = pd.concat([p_desc, df], axis=1)\n",
    "\n",
    "# drop description to prevent duplicate columns\n",
    "transactions_processed = transactions_processed.drop(['detail_desc'], axis=1)\n",
    "\n",
    "# merge dataframe with transactions\n",
    "transactions_processed = transactions_processed.merge(df, on='prod_name')\n",
    "\n",
    "# drop product name and description\n",
    "transactions_processed = transactions_processed.drop(['prod_name', 'detail_desc'], axis=1)\n",
    "\n",
    "# evaluate\n",
    "evaluate(transactions_processed, vec_size2)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# W2V D2V Together\n",
    "\n",
    "Evaluation may or may not work. Copying the code to a py file works more reliably.\n",
    "Reducing the vector sizes in the previous models is almost a requirement, since 100 dimensional vectors will not be classified. A lenght of 25 for each vector seems like a good starting point.\n",
    "***Requires trained models from previous parts.***"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "outputs": [],
   "source": [
    "print(\"Starting implementation combination\")\n",
    "\n",
    "# prepare transactions\n",
    "# same as for the baseline\n",
    "transactions_processed = transactions[\n",
    "    ['customer_id', 'age', 'article_id', 'prod_name', 'sales_channel_id', 'price', 'ordered', 'detail_desc']].copy()\n",
    "customer_encoder = preprocessing.LabelEncoder()\n",
    "article_encoder = preprocessing.LabelEncoder()\n",
    "transactions_processed['customer_id'] = customer_encoder.fit_transform(transactions_processed['customer_id'])\n",
    "transactions_processed['article_id'] = article_encoder.fit_transform(transactions_processed['article_id'])\n",
    "transactions_processed = transactions_processed.fillna(0)\n",
    "transactions_processed.isnull().values.any()\n",
    "transactions_processed = pd.get_dummies(transactions_processed, columns=['sales_channel_id'])\n",
    "\n",
    "# df to loop over\n",
    "p_desc = pd.DataFrame(transactions_processed[['prod_name', 'detail_desc']]).drop_duplicates().reset_index(drop=True, inplace=False)\n",
    "\n",
    "# make a vector for each product\n",
    "print(\"Starting vectorization\")\n",
    "v = np.empty((0, vec_size1 + vec_size2))\n",
    "for i, row in enumerate(p_desc.values):\n",
    "    p_name = row[0]\n",
    "    desc = row[1]\n",
    "    desc = word_tokenize(\n",
    "        str(desc).lower().translate(str.maketrans(string.punctuation, ' ' * len(string.punctuation))))\n",
    "    desc_vec = model2.infer_vector(desc)\n",
    "    name_vec = model.wv[p_name]\n",
    "    vec = np.concatenate((name_vec, desc_vec), axis=None)\n",
    "    v = np.append(v, [vec], axis=0)\n",
    "    if i % 1000 == 0:\n",
    "        print(f\"\\rProcessed {i} / {p_desc.shape[0]} ({i/p_desc.shape[0]*100:.2f}%) rows\", end=\"\")\n",
    "print()\n",
    "\n",
    "# make a dataframe out of vectors and\n",
    "df = pd.DataFrame(v, columns=[f'f_{i}' for i in range(vec_size1 + vec_size2)])\n",
    "df = pd.concat([p_desc, df], axis=1)\n",
    "\n",
    "# drop column to prevent duplicate\n",
    "transactions_processed = transactions_processed.drop(['detail_desc'], axis=1)\n",
    "# merge dataframe with transactions\n",
    "transactions_processed = transactions_processed.merge(df, on='prod_name')\n",
    "# drop name and description\n",
    "transactions_processed = transactions_processed.drop(['prod_name', 'detail_desc'], axis=1)\n",
    "#evaluate\n",
    "evaluate(transactions_processed, vec_size1+vec_size2)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Recent popularity\n",
    "\n",
    "Somewhat slow"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Long loop\n",
      "Processed 1590000 / 1598761 rows\n",
      "Processed 3190000 / 3197522 (99.76%) rows\n",
      "         customer_id   age  article_id     price  ordered      t_dat  \\\n",
      "0                 14  22.0        4029  0.016932        1 2018-09-20   \n",
      "381716         48660  58.0       26263  0.033881        1 2018-09-20   \n",
      "1342250        67720  20.0         805  0.013542        0 2018-09-20   \n",
      "1342705        33681  27.0       11446  0.022017        1 2018-09-20   \n",
      "1342746        33681  27.0       13443  0.025407        1 2018-09-20   \n",
      "1342771        33681  27.0       11585  0.016932        1 2018-09-20   \n",
      "1861668        62179  21.0       22937  0.011847        1 2018-09-20   \n",
      "2718627        52818  26.0       17134  0.016932        0 2018-09-20   \n",
      "1343424        33779  50.0       21043  0.030492        1 2018-09-20   \n",
      "1343525        33779  50.0       25058  0.022864        1 2018-09-20   \n",
      "\n",
      "         sales_channel_id_1  sales_channel_id_2  rec_purchases  \n",
      "0                         0                   1              2  \n",
      "381716                    0                   1              2  \n",
      "1342250                   1                   0              0  \n",
      "1342705                   0                   1              0  \n",
      "1342746                   0                   1              0  \n",
      "1342771                   0                   1              0  \n",
      "1861668                   0                   1              0  \n",
      "2718627                   0                   1              0  \n",
      "1343424                   1                   0              0  \n",
      "1343525                   1                   0              0  \n",
      "Evaluating...\n",
      "              age     price  ordered  sales_channel_id_1  sales_channel_id_2  \\\n",
      "0       -1.001027 -0.553526        1                   0                   1   \n",
      "381716   1.575002  0.157003        1                   0                   1   \n",
      "1342250 -1.144140 -0.695632        0                   1                   0   \n",
      "1342705 -0.643246 -0.340368        1                   0                   1   \n",
      "1342746 -0.643246 -0.198262        1                   0                   1   \n",
      "...           ...       ...      ...                 ...                 ...   \n",
      "1906761 -0.857915 -0.553526        0                   0                   1   \n",
      "956111  -0.929471 -0.553526        1                   1                   0   \n",
      "2720285 -0.643246  0.849768        0                   0                   1   \n",
      "389108  -0.500133  0.157003        0                   1                   0   \n",
      "2081699  0.930995  1.222796        1                   0                   1   \n",
      "\n",
      "         rec_purchases  \n",
      "0            -0.147677  \n",
      "381716       -0.147677  \n",
      "1342250      -0.449337  \n",
      "1342705      -0.449337  \n",
      "1342746      -0.449337  \n",
      "...                ...  \n",
      "1906761      -0.449337  \n",
      "956111       -0.449337  \n",
      "2720285      -0.449337  \n",
      "389108       -0.449337  \n",
      "2081699      -0.147677  \n",
      "\n",
      "[3197522 rows x 6 columns]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.78      0.94      0.85    159830\n",
      "           1       0.92      0.74      0.82    159923\n",
      "\n",
      "    accuracy                           0.84    319753\n",
      "   macro avg       0.85      0.84      0.84    319753\n",
      "weighted avg       0.85      0.84      0.84    319753\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# prepare transactions\n",
    "# same as for the baseline\n",
    "transactions_processed = transactions[\n",
    "    ['customer_id', 'age', 'article_id', 'sales_channel_id', 'price', 'ordered', 't_dat']].copy()\n",
    "customer_encoder = preprocessing.LabelEncoder()\n",
    "article_encoder = preprocessing.LabelEncoder()\n",
    "transactions_processed['customer_id'] = customer_encoder.fit_transform(transactions_processed['customer_id'])\n",
    "transactions_processed['article_id'] = article_encoder.fit_transform(transactions_processed['article_id'])\n",
    "transactions_processed = transactions_processed.fillna(0)\n",
    "transactions_processed.isnull().values.any()\n",
    "transactions_processed = pd.get_dummies(transactions_processed, columns=['sales_channel_id'])\n",
    "\n",
    "# process dates\n",
    "transactions_processed['t_dat'] = pd.to_datetime(transactions_processed['t_dat'], format='%Y-%m-%d')\n",
    "transactions_processed = transactions_processed.sort_values(by=['t_dat'])\n",
    "\n",
    "# make dict of purchase dates\n",
    "purchase_dates = {}\n",
    "purchases = transactions_processed[transactions_processed['ordered'] == 1]  # only keep purchases\n",
    "purchases = purchases.sort_values(by=['t_dat'])\n",
    "purchases = purchases[['article_id', 't_dat']]\n",
    "for index, row in enumerate(purchases.values):\n",
    "    article_id = row[0]\n",
    "    date = row[1]\n",
    "    if article_id not in purchase_dates:\n",
    "        purchase_dates[article_id] = []\n",
    "    purchase_dates[article_id].append(date)\n",
    "    if index % 10000 == 0:\n",
    "        print(f\"\\rProcessed {index} / {purchases.shape[0]} rows\", end=\"\")\n",
    "print()\n",
    "\n",
    "# make a list of #recent purchases\n",
    "# takes a while\n",
    "rec_purchase_num = []\n",
    "for index, row in enumerate(transactions_processed.values):\n",
    "    article_id = row[2]\n",
    "    date = row[5]\n",
    "    if article_id in purchase_dates:\n",
    "        dates = purchase_dates[article_id]\n",
    "        dates = [d for d in dates if (date >= d >= date - pd.Timedelta(days=7))]\n",
    "        rec_purchase_num.append(len(dates) - row[4])\n",
    "    else:\n",
    "        rec_purchase_num.append(0)\n",
    "    if index % 10000 == 0:\n",
    "        print(f\"\\rProcessed {index} / {transactions_processed.shape[0]} ({index/transactions_processed.shape[0]*100:.2f}%) rows\", end=\"\")\n",
    "print()\n",
    "\n",
    "# add list to transactions\n",
    "transactions_processed['rec_purchases'] = rec_purchase_num\n",
    "print(transactions_processed.head(10))\n",
    "\n",
    "# drop dates\n",
    "transactions_processed = transactions_processed.drop(['t_dat'], axis=1)\n",
    "# evaluate\n",
    "evaluate(transactions_processed, vec_size=0, extra_vec=['rec_purchases'])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# W2V and Popularity\n",
    "\n",
    "Evaluation may or may not work. Copying the code to a py file works more reliably.\n",
    "Reducing vector size may work (but give worse performance).\n",
    "Requires trained models from previous parts."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "vec_size = 50\n",
    "\n",
    "print(\"Starting implementation combination w2v, pop\")\n",
    "\n",
    "# prepare transactions\n",
    "transactions_processed = transactions[\n",
    "    ['customer_id', 'age', 'article_id', 'sales_channel_id', 'price', 'ordered', 't_dat', 'prod_name']].copy()\n",
    "customer_encoder = preprocessing.LabelEncoder()\n",
    "article_encoder = preprocessing.LabelEncoder()\n",
    "transactions_processed['customer_id'] = customer_encoder.fit_transform(transactions_processed['customer_id'])\n",
    "transactions_processed['article_id'] = article_encoder.fit_transform(transactions_processed['article_id'])\n",
    "transactions_processed = transactions_processed.fillna(0)\n",
    "transactions_processed.isnull().values.any()\n",
    "transactions_processed = pd.get_dummies(transactions_processed, columns=['sales_channel_id'])\n",
    "\n",
    "transactions_processed['t_dat'] = pd.to_datetime(transactions_processed['t_dat'], format='%Y-%m-%d')\n",
    "transactions_processed = transactions_processed.sort_values(by=['t_dat'])\n",
    "\n",
    "# w2v part\n",
    "# generate vectors for all product names\n",
    "print(\"Starting vectorization\")\n",
    "p_names = pd.DataFrame(transactions_processed['prod_name']).drop_duplicates().reset_index(drop=True, inplace=False)\n",
    "v = np.empty((0, vec_size))\n",
    "for a in p_names.values:\n",
    "    vec = model.wv.__getitem__([a[0]])\n",
    "    v = np.append(v, vec, axis=0)\n",
    "df = pd.DataFrame(v, columns=[f'f_{i}' for i in range(vec_size)])\n",
    "df = pd.concat([p_names, df], axis=1)\n",
    "\n",
    "# pop part\n",
    "# make dict of purchase dates\n",
    "purchase_dates = {}\n",
    "purchases = transactions_processed[transactions_processed['ordered'] == 1]  # only keep purchases\n",
    "purchases = purchases.sort_values(by=['t_dat'])\n",
    "purchases = purchases[['article_id', 't_dat']]\n",
    "for index, row in enumerate(purchases.values):\n",
    "    article_id = row[0]\n",
    "    date = row[1]\n",
    "    if article_id not in purchase_dates:\n",
    "        purchase_dates[article_id] = []\n",
    "    purchase_dates[article_id].append(date)\n",
    "    if index % 10000 == 0:\n",
    "        print(f\"\\rProcessed {index} / {purchases.shape[0]} rows\", end=\"\")\n",
    "print()\n",
    "\n",
    "# make a list of #recent purchases\n",
    "rec_purchase_num = []\n",
    "for index, row in enumerate(transactions_processed.values):\n",
    "    article_id = row[2]\n",
    "    date = row[5]\n",
    "    if article_id in purchase_dates:\n",
    "        dates = purchase_dates[article_id]\n",
    "        dates = [d for d in dates if (date >= d >= date - pd.Timedelta(days=7))]\n",
    "        rec_purchase_num.append(len(dates) - row[4])\n",
    "    else:\n",
    "        rec_purchase_num.append(0)\n",
    "    if index % 10000 == 0:\n",
    "        print(f\"\\rProcessed {index} / {transactions_processed.shape[0]} ({index/transactions_processed.shape[0]*100:.2f}%) rows\", end=\"\")\n",
    "\n",
    "print()\n",
    "\n",
    "# add recent purchases to frame\n",
    "transactions_processed['rec_purchases'] = rec_purchase_num\n",
    "# merge with product name vectors\n",
    "transactions_processed = transactions_processed.merge(df, on='prod_name')\n",
    "# remove helper columns\n",
    "transactions_processed = transactions_processed.drop(['t_dat'], axis=1)\n",
    "transactions_processed = transactions_processed.drop(['prod_name'], axis=1)\n",
    "# evaluate\n",
    "evaluate(transactions_processed, vec_size=vec_size, extra_vec=['rec_purchases'])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}