{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Recommendation - voting system\n",
    "For each article in the user’s history, get the `NR_SIMILAR` most similar articles. For each article in the pool of selected similar articles, calculate a final similarity score by adding up all similarities to all articles in the user’s history (if they are in the similarities dataframe, which means they should be in the top `NR_TO_KEEP` (see `similarities.ipynb`) similar articles)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import threading\n",
    "from functions import *"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "NR_ROWS_PER_THREAD = 50000                                                              # nr of customers to process in each thread\n",
    "NR_SIMILAR = 3                                                                          # get the top NR_SIMILAR articles for each article\n",
    "SIMILARITY_ARTICLE_ID_FILE = ''                                                         # input file (containing the similar article ids)\n",
    "SIMILARITY_SCORE_FILE = SIMILARITY_ARTICLE_ID_FILE.replace('article_ids', 'indices')    # input file (containing the similarity scores)\n",
    "OUTPUT_FILE_NAME = f'submission_voting_system_{NR_SIMILAR}.csv'                         # name of the submission file\n",
    "VALIDATE_CUSTOMER = 3"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "article_df = pd.read_feather(idp(filename='articles_processed.feather'))\n",
    "no_image_article_ids = article_df[article_df['image_name'] == 'does not exist']['article_id'].values.tolist()\n",
    "most_popular_article_ids = article_df.sort_values(by='popularity', ascending=False).head(12)['article_id'].values.tolist()\n",
    "\n",
    "customers_transactions_df = pd.read_feather(idp(filename='customers_transactions_processed.feather'))\n",
    "nr_customers = customers_transactions_df.shape[0]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Submission creation pipeline\n",
    "The pipeline uses multithreading to reduce the execution time."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def get_recommendations_voting_system(customer_purchase_hist, similar_article_dict, similar_article_score):\n",
    "    \"\"\"\n",
    "    Get the recommendations for a single customer based on its purchase history.\n",
    "    :param customer_purchase_hist: a string with space-separated article_ids, representing the purchase history (first = least recently purchased, last = most recently purchased)\n",
    "    :param similar_article_dict: a dictionary mapping each article to a tuple with NR_TO_KEEP (see similarities.ipynb) article_ids that are considered as the 50 most similar articles\n",
    "    :param similar_article_score: a dictionary mapping each article to a tuple with NR_TO_KEEP (see similarities.ipynb) values that correspond to the similarity scores\n",
    "    :return: a string with 12 recommendations\n",
    "    \"\"\"\n",
    "    # remove all duplicates and all article_ids that are not linked to an image\n",
    "    customer_purchase_hist = list(dict.fromkeys(\n",
    "        [c for c in article_str_to_list(article_id_str=customer_purchase_hist) if c not in no_image_article_ids]\n",
    "    ))\n",
    "    # create a candidate list\n",
    "    candidate_list = most_popular_article_ids.copy()\n",
    "    for i in range(3):\n",
    "        candidate_list.extend([similar_article_dict[history][i] for history in customer_purchase_hist])\n",
    "\n",
    "    article_sim_pairs = [\n",
    "        t\n",
    "        for article_id in customer_purchase_hist\n",
    "        for t in list(zip(similar_article_dict[article_id], similar_article_score[article_id]))\n",
    "        if t[0] in candidate_list\n",
    "    ]\n",
    "    candidate_dict = {\n",
    "        candidate: sum([x[1] for x in article_sim_pairs if x[0] == candidate])\n",
    "        for candidate in candidate_list\n",
    "    }\n",
    "    final_candidates = [y[0] for y in sorted(list(candidate_dict.items()), key=lambda x: x[1], reverse=True)]\n",
    "\n",
    "    return article_list_to_str(article_id_list=final_candidates[:12])\n",
    "\n",
    "def create_voting_system_recommendations_thread_function(min_row_ind, max_row_ind, thread_nr, similar_article_dict, similar_article_score):\n",
    "    \"\"\"\n",
    "    Apply the get_recommendations_advanced_similarity function to a batch of customers (defined by min_row_ind and max_row_ind).\n",
    "    Write the result to a temporary partial submission file.\n",
    "    :param min_row_ind: smallest row index in the range to retrieve\n",
    "    :param max_row_ind: largest row index in the range to retrieve\n",
    "    :param thread_nr: nr of the thread\n",
    "    :param similar_article_dict: a dictionary mapping each article to a tuple with 50 article_ids that are considered as the 50 most similar articles\n",
    "    :param similar_article_score: a dictionary mapping each article to a tuple with 50 scores that correspond to the similarity scores\n",
    "    \"\"\"\n",
    "\n",
    "    print(f\"[=>    ] Started              : Thread {thread_nr} ({min_row_ind} --> {max_row_ind})\")\n",
    "    part_of_df = customers_transactions_df.iloc[min_row_ind:max_row_ind].copy()\n",
    "    part_of_df['prediction'] = part_of_df['purchase_history'].apply(\n",
    "        lambda hist: get_recommendations_voting_system(\n",
    "            customer_purchase_hist=hist, similar_article_dict=similar_article_dict, similar_article_score=similar_article_score\n",
    "        )\n",
    "    )\n",
    "    part_of_submission = part_of_df[['customer_id', 'prediction']]\n",
    "    part_of_submission.to_csv(submission_odp(filename=f'submission_{thread_nr}.csv', creation=True), index=False)\n",
    "    print(f\"[=====>] Finished             : Thread {thread_nr} ({min_row_ind} --> {max_row_ind})\")\n",
    "    return\n",
    "\n",
    "def run_threaded_advanced_similarity_recommendations():\n",
    "    \"\"\"\n",
    "    Find recommendations for all customers in the dataset\n",
    "    :param extended: use the similarities based on the extended embeddings\n",
    "    \"\"\"\n",
    "\n",
    "    min_row_ind, max_row_ind = 0, NR_ROWS_PER_THREAD     # lower and upperbound of rows to extract within a thread\n",
    "    thread_nr = 1                                        # only for progress printing\n",
    "    threads = list()\n",
    "\n",
    "    similarity_df_article_ids = pd.read_feather(odp(filename=SIMILARITY_ARTICLE_ID_FILE))\n",
    "    similarity_article_dict = get_similar_article_dict(\n",
    "        similarity_df_article_ids=similarity_df_article_ids,\n",
    "        similarity_levels=list(range(1, similarity_df_article_ids.shape[1]))\n",
    "    )\n",
    "    similarity_score_dict = get_similar_score_dict(\n",
    "        similarity_df_article_ids=similarity_df_article_ids,\n",
    "        similarity_df_sim=pd.read_feather(odp(filename=SIMILARITY_SCORE_FILE)),\n",
    "        similarity_levels=list(range(1, similarity_df_article_ids.shape[1]))\n",
    "    )\n",
    "\n",
    "    # create threads\n",
    "    while nr_customers > min_row_ind:\n",
    "        print(\"Main    : created and started thread %d\", thread_nr)\n",
    "        # create and start thread\n",
    "        thread = threading.Thread(\n",
    "            target=create_voting_system_recommendations_thread_function,\n",
    "            args=(min_row_ind, max_row_ind, thread_nr, similarity_article_dict, similarity_score_dict)\n",
    "        )\n",
    "        threads.append(thread)\n",
    "        thread.start()\n",
    "        # update parameters\n",
    "        min_row_ind, max_row_ind = max_row_ind, min(nr_customers, max_row_ind + NR_ROWS_PER_THREAD)\n",
    "        thread_nr += 1\n",
    "\n",
    "    # join threads\n",
    "    for thread_index, thread in enumerate(threads):\n",
    "        print(\"Main    : next thread to join: %d.\", thread_index + 1)\n",
    "        thread.join()\n",
    "        print(\"Main    : thread %d done\", thread_index + 1)\n",
    "\n",
    "    return thread_nr - 1"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "nr_threads = run_threaded_advanced_similarity_recommendations()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "join_partial_submissions(\n",
    "    base_file_name=submission_odp(filename=f'submission_*.csv', creation=True),\n",
    "    trgt_file_name=submission_odp(OUTPUT_FILE_NAME),\n",
    "    nr_files=nr_threads\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "remove_directory_if_exists(directory=odp(filename='submission_creation'))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Subjective validation\n",
    "Check the format of the submission file and visually assess the recommendations."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# check whether the submission file is valid\n",
    "check_submission(\n",
    "    filename=submission_odp(filename=OUTPUT_FILE_NAME),\n",
    "    nr_customers=nr_customers\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# randomly show any customer case\n",
    "random_show_customer_case(\n",
    "    filename=submission_odp(filename=OUTPUT_FILE_NAME),\n",
    "    customer_transactions_df=customers_transactions_df\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# show the customer at index VALIDATE_CUSTOMER\n",
    "show_customer_case(\n",
    "    customer_id=VALIDATE_CUSTOMER,\n",
    "    filename=submission_odp(filename=OUTPUT_FILE_NAME),\n",
    "    customer_transactions_df=customers_transactions_df\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "%reset -f"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}