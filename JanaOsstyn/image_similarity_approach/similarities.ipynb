{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Similarities\n",
    "When embeddings are created, similarity between images can be calculated. This needs to be done in batches as constructing a dataframe of shape `nr_articles x nr_articles` is too large to fit in memory. I created batches of `NR_ROWS_PER_BATCH` articles each. Each batch is written to a feather file. Afterwards, these feather files are trimmed to only keep the `NR_TO_KEEP` most similar articles. Two new csv files are created: one containing the indices of the `NR_TO_KEEP` most similar articles, while the other one contains the corresponding similarity values. The dimensions of the dataframes in these csv files are now small enough to join them into a single csv file per file type (one for the similarity values and one for the indices). Finally, it is more interesting to have a dataframe with similar `article_ids` rather than a dataframe with the indices of similar articles, so the indices are converted to their corresponding `article_id`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import math, threading\n",
    "\n",
    "from functions import idp, odp, create_directory_if_not_exists, list_directory_if_exists\n",
    "\n",
    "from sklearn.metrics import pairwise_distances"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "NR_ROWS_PER_BATCH = 1000        # nr of rows to process in each batch in the calculation phase\n",
    "NR_FILES_PER_THREAD = 10        # nr of files per thread in the trimming phase\n",
    "NR_TO_KEEP = 100                # nr of similar articles to store for each article_id"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def similarities_odp(filename, creation=False):\n",
    "    \"\"\"\n",
    "    Get the filename including the path to store/open a file containing similarities.\n",
    "    :param filename: the filename of the file\n",
    "    :param creation: if True, the directory is 'similarities_creation' instead of 'similarities'\n",
    "    :return: the filename with path\n",
    "    \"\"\"\n",
    "    directory = 'similarities_creation' if creation else 'similarities'\n",
    "    create_directory_if_not_exists(directory=directory)\n",
    "    return odp(filename=f'{directory}/{filename}')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "embeddings_directories = list_directory_if_exists(directory=odp(filename='embeddings'))\n",
    "similarity_directories = list_directory_if_exists(directory=odp(filename='similarities'))\n",
    "\n",
    "directories_to_process = [\n",
    "    emb_filename for emb_filename in sorted(list(\n",
    "        set([sim_filename.replace('embeddings', 'similarities') for sim_filename in embeddings_directories]) - set(similarity_directories)\n",
    "    ))\n",
    "]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Similarity pipeline\n",
    "### Calculate similarities in batches"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "article_df = pd.read_feather(idp(filename='articles_processed.feather'))\n",
    "nr_articles = article_df.shape[0]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def calculate_similarities_for_batch(min_row_ind, max_row_ind, batch_nr, embeddings_df):\n",
    "    \"\"\"\n",
    "    Extract the rows of the dataframe from index min_ind to index max_ind.\n",
    "    Then, calculate pairwise similarities.\n",
    "    The similarities for all selected rows are written to a file numbered by batch_nr.\n",
    "    :param min_row_ind: smallest row index in the range to retrieve\n",
    "    :param max_row_ind: largest row index in the range to retrieve\n",
    "    :param batch_nr: nr of the batch (printing purposes only)\n",
    "    :param embeddings_df: dataframe containing the embeddings\n",
    "    \"\"\"\n",
    "    print(f\"[=>    ] Started              : Batch {batch_nr} ({min_row_ind} --> {max_row_ind})\")\n",
    "    similarities = 1 - pairwise_distances(embeddings_df.iloc[min_row_ind:max_row_ind], embeddings_df, metric='cosine')\n",
    "    similarity_df = pd.DataFrame(similarities)\n",
    "    similarity_df['article_index'] = list(range(min_row_ind, max_row_ind))\n",
    "    similarity_df.columns = similarity_df.columns.astype(str)\n",
    "    similarity_df.to_feather(similarities_odp(filename=f'similarities_{batch_nr}.feather', creation=True))\n",
    "    print(f\"[=====>] Finished             : Batch {batch_nr} ({min_row_ind} --> {max_row_ind})\")\n",
    "    return\n",
    "\n",
    "def run_batch_similarity_calculation(embeddings_df):\n",
    "    \"\"\"\n",
    "    Calculate the similarities for all embeddings.\n",
    "    Calculating the similarities for all embeddings at once requires a matrix with size nr_articles x nr_articles to be created, which is impossible with limited ram.\n",
    "    Therefore, the calculation is split into batches of similarity_step rows.\n",
    "    For each batch, the similarities are stored in a file numbered by batch_nr.\n",
    "    :param embeddings_df: dataframe containing the embeddings\n",
    "    \"\"\"\n",
    "\n",
    "    min_row_ind, max_row_ind = 0, NR_ROWS_PER_BATCH      # lower and upperbound of rows to extract within a thread\n",
    "    batch_nr = 1                                         # only for progress printing\n",
    "\n",
    "    while min_row_ind <= article_df.shape[0] and min_row_ind != max_row_ind:\n",
    "        # similarity calculation\n",
    "        calculate_similarities_for_batch(min_row_ind=min_row_ind, max_row_ind=max_row_ind, batch_nr=batch_nr, embeddings_df=embeddings_df)\n",
    "        # update parameters\n",
    "        min_row_ind, max_row_ind = max_row_ind, min(nr_articles, max_row_ind + NR_ROWS_PER_BATCH)\n",
    "        batch_nr += 1\n",
    "\n",
    "    return"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Trim similarities using multithreading"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def trim_similarities_of_file(file_name, column_names, trimmed_column_names):\n",
    "    \"\"\"\n",
    "    The similarities are divided over multiple files, which makes a simple lookup more complex.\n",
    "    However, there is no need to keep *all* similarities, we usually don't want to do anything with the articles that are not similar at all.\n",
    "    Therefore, only keep the nr_to_keep most similar articles (and the similarity score) for each article.\n",
    "    The data is split into:\n",
    "        - an indices dataframe  (giving information about the indices of similar articles within the article dataframe)\n",
    "        - a values dataframe    (giving information about how similar articles are)\n",
    "    Two new files are created to store this information.\n",
    "    :param file_name: the name of the batch file to process.\n",
    "    :param column_names: column names to take into account in the original dataframe\n",
    "    :param trimmed_column_names: column names for the trimmed dataframe\n",
    "    \"\"\"\n",
    "    similarities_df = pd.read_feather(similarities_odp(filename=file_name, creation=True))\n",
    "    similarities_df = similarities_df[column_names].T\n",
    "\n",
    "    result_values, result_indices = [], []\n",
    "\n",
    "    # obtain all similar article indices and their corresponding similarity score\n",
    "    for i in range(similarities_df.shape[1]):\n",
    "        x = similarities_df[i].sort_values(ascending=False).head(NR_TO_KEEP)\n",
    "        result_values.append(x.values)\n",
    "        result_indices.append(x.index)\n",
    "\n",
    "    # convert to dataframes\n",
    "    sim_df = pd.DataFrame(data=result_values, columns=trimmed_column_names, index=[i for i in range(len(result_values))])\n",
    "    ind_df = pd.DataFrame(data=result_indices, columns=trimmed_column_names, index=[i for i in range(len(result_indices))])\n",
    "    sim_df = sim_df.reset_index()\n",
    "    ind_df = ind_df.reset_index()\n",
    "    sim_df.columns = sim_df.columns.astype(str)\n",
    "    ind_df.columns = ind_df.columns.astype(str)\n",
    "\n",
    "    # store the dataframes\n",
    "    sim_df.to_feather(similarities_odp(filename=file_name.replace('.feather', '_values.feather'), creation=True))\n",
    "    ind_df.to_feather(similarities_odp(filename=file_name.replace('.feather', '_indices.feather'), creation=True))\n",
    "    return\n",
    "\n",
    "def similarity_trimming_thread_function(filename_list, thread_nr, column_names, trimmed_column_names):\n",
    "    \"\"\"\n",
    "    Execute the trim_similarities_of_file function for each file whose name is in the filename_list\n",
    "    :param filename_list: a list of filenames\n",
    "    :param thread_nr: the number of the thread\n",
    "    :param column_names: column names to take into account in the original dataframe\n",
    "    :param trimmed_column_names: column names for the trimmed dataframe\n",
    "    \"\"\"\n",
    "    for filename in filename_list:\n",
    "        print(f\"[=>    ] Started              : Thread {thread_nr} - file {filename}\")\n",
    "        trim_similarities_of_file(file_name=filename, column_names=column_names, trimmed_column_names=trimmed_column_names)\n",
    "        print(f\"[=====>] Finished             : Thread {thread_nr} - file {filename}\")\n",
    "    return\n",
    "\n",
    "def run_threaded_similarity_trimming():\n",
    "    \"\"\"\n",
    "    To speed up calculations, split the trimming of similarity over different threads, where each thread processes files_per_thread files.\n",
    "    \"\"\"\n",
    "\n",
    "    # divide the filenames in batches of files_per_thread filenames\n",
    "    filenames = [f'similarities_{i + 1}.feather' for i in range(math.ceil(nr_articles / NR_ROWS_PER_BATCH))]\n",
    "\n",
    "    nr_of_full_batches = len(filenames) // NR_ROWS_PER_BATCH\n",
    "    thread_filenames = \\\n",
    "        [filenames[i * NR_FILES_PER_THREAD:(i + 1) * NR_FILES_PER_THREAD] for i in range(nr_of_full_batches)] + \\\n",
    "        [filenames[nr_of_full_batches * NR_FILES_PER_THREAD:]]\n",
    "\n",
    "    column_names = [str(i) for i in range(nr_articles)]             # there is one column for each article, with column names 0, 1, 2,... (article indices)\n",
    "    trimmed_column_names = [str(i) for i in range(NR_TO_KEEP)]      # same as columns but as we only keep the NR_TO_KEEP most similar articles\n",
    "\n",
    "    # create threads\n",
    "    nr_threads = len(thread_filenames)\n",
    "    threads = list()\n",
    "    for thread_index in range(nr_threads):\n",
    "        print(\"Main    : created and started thread %d.\", thread_index + 1)\n",
    "        thread = threading.Thread(\n",
    "            target=similarity_trimming_thread_function,\n",
    "            args=(thread_filenames[thread_index], thread_index + 1, column_names, trimmed_column_names,)\n",
    "        )\n",
    "        threads.append(thread)\n",
    "        thread.start()\n",
    "\n",
    "    # join threads\n",
    "    for thread_index, thread in enumerate(threads):\n",
    "        print(\"Main    : next thread to join: %d\", thread_index + 1)\n",
    "        thread.join()\n",
    "        print(\"Main    : thread %d done\", thread_index + 1)\n",
    "\n",
    "    return"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Join similarities"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def join_similarity_files(sim_type, extended=False):\n",
    "    \"\"\"\n",
    "    Join the similarity files of type indices or values into a single file containing all similarities of the 50 most similar articles\n",
    "    :param sim_type: either 'indices' or 'values'.\n",
    "    :param extended: additional to the image embedding, article properties were added\n",
    "    \"\"\"\n",
    "    similarities_list = [\n",
    "        pd.read_feather(similarities_odp(filename=f'similarities_{i + 1}_{sim_type}.feather', creation=True))\n",
    "        for i in range(math.ceil(nr_articles / NR_ROWS_PER_BATCH))\n",
    "    ]\n",
    "    for i, sim in enumerate(similarities_list):\n",
    "        sim.rename(columns={'Unnamed: 0': 'article_index'}, inplace=True)\n",
    "        sim['article_index'] = NR_ROWS_PER_BATCH * i + sim['article_index']\n",
    "    all_similarities = pd.concat(similarities_list, ignore_index=True)\n",
    "    if 'Unnamed: 0' in all_similarities.columns.values:\n",
    "        all_similarities = all_similarities.drop(['Unnamed: 0'], axis=1)\n",
    "    if 'index' in all_similarities.columns.values:\n",
    "        all_similarities = all_similarities.drop(['index'], axis=1)\n",
    "    all_similarities.to_feather(similarities_odp(filename=f'{\"extended_\" if extended else \"\"}similarities_{sim_type}.feather'))\n",
    "    return"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Convert indices to article_ids"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def convert_indices_to_article_ids(filename):\n",
    "    \"\"\"\n",
    "    For similarity calculation, indices of the articles were kept to identify an article. However, it makes things easier further on if we could work with article_ids directly. The indices are in this function converted to their corresponding article id.\n",
    "    :param filename: the name of the file that contains the indices\n",
    "    :return: the dataframe containing the article_ids instead of the indices\n",
    "    \"\"\"\n",
    "    article_ids = article_df['article_id'].tolist()\n",
    "    article_lookup = {i: article_id for i, article_id in enumerate(article_ids)}  # translation dictionary\n",
    "    similarity_ind_df = pd.read_feather(filename)\n",
    "\n",
    "    similarity_ind_df['article_index'] = similarity_ind_df['article_index'].apply(lambda x: article_lookup[x])\n",
    "    for i in range(similarity_ind_df.shape[1] - 1):\n",
    "        similarity_ind_df[str(i)] = similarity_ind_df[str(i)].apply(lambda x: article_lookup[x])\n",
    "    similarity_ind_df.to_feather(filename.replace('indices', 'article_ids'))\n",
    "    return"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Assembly\n",
    "The whole pipeline is defined, now execute it for each embedding file that is not yet processed."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def process_embedding_file(filename):\n",
    "    \"\"\"\n",
    "    Execute the similarity calculation pipeline for a single file\n",
    "    :param filename: name of the file containing the embeddings\n",
    "    \"\"\"\n",
    "    print(f'START {filename} >>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>')\n",
    "    embeddings_df = pd.read_feather(filename)\n",
    "    print(f'DONE step 1 / 6 ---------------------------------------------------------------------------------------------------------------------------------------')\n",
    "    run_batch_similarity_calculation(embeddings_df=embeddings_df)\n",
    "    print(f'DONE step 2 / 6 ---------------------------------------------------------------------------------------------------------------------------------------')\n",
    "    run_threaded_similarity_trimming()\n",
    "    print(f'DONE step 3 / 6 ---------------------------------------------------------------------------------------------------------------------------------------')\n",
    "    join_similarity_files(sim_type='values', extended='extended' in filename)\n",
    "    print(f'DONE step 4 / 6 ---------------------------------------------------------------------------------------------------------------------------------------')\n",
    "    join_similarity_files(sim_type='indices', extended='extended' in filename)\n",
    "    print(f'DONE step 5 / 6 ---------------------------------------------------------------------------------------------------------------------------------------')\n",
    "    convert_indices_to_article_ids(filename=similarities_odp(filename=f'{\"extended_\" if \"extended\"  in filename else \"\"}similarities_indices.feather'))\n",
    "    print(f'DONE step 6 / 6 ---------------------------------------------------------------------------------------------------------------------------------------')\n",
    "    print(f'END {filename} >>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>')\n",
    "    return"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "for directory_to_process in directories_to_process:\n",
    "    embedding_files = [\n",
    "        odp(filename=f'embeddings/{directory_to_process}/embeddings.feather'),\n",
    "        odp(filename=f'embeddings/{directory_to_process}/extended_embeddings.feather')\n",
    "    ]\n",
    "    for embedding_file in embedding_files:\n",
    "        process_embedding_file(filename=embedding_file)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "%reset -f"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}