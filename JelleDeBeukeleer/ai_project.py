# -*- coding: utf-8 -*-
"""AI_project.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1-F1iROZQnoavh8LVEGkdE01XiddWPY2C
"""

import pandas as pd
import numpy as np
import random
import datetime
from sklearn import preprocessing
from sklearn.linear_model import LogisticRegression
from sklearn.neural_network import MLPClassifier, MLPRegressor
from sklearn.tree import DecisionTreeClassifier
from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report
import gc

articles_filename = '../data/articles_pruned.csv'
customers_filename = '../data/customers_pruned.csv'
transactions_filename = '../data/transactions_train_pruned.csv'

gc.collect()
articles = pd.read_csv(articles_filename)
customers = pd.read_csv(customers_filename)
transactions = pd.read_csv(transactions_filename)
customer_encoder = preprocessing.LabelEncoder()
article_encoder = preprocessing.LabelEncoder()
customers['customer_id'] = customer_encoder.fit_transform(customers['customer_id'])
articles['article_id'] = article_encoder.fit_transform(articles['article_id'])
transactions['customer_id'] = customer_encoder.transform(transactions['customer_id'])
transactions['article_id'] = article_encoder.transform(transactions['article_id'])
transactions['ordered'] = 1

print(len(customers))

print("taking recent slice")
n_weeks = 4
last_day = transactions['t_dat'].max()
last_day_date = [int(i) for i in last_day.split('-')]
last_day_date = datetime.date(last_day_date[0], last_day_date[1], last_day_date[2])
min_date = last_day_date - datetime.timedelta(weeks=n_weeks)
min_date = min_date.strftime('%Y-%m-%d')
recent_slice = transactions[transactions['t_dat'] >= min_date].copy()

print("calculating user purchase count")
recent_purchases = recent_slice.groupby('customer_id')[['article_id']].count()
recent_purchases.rename(columns={'article_id': 'customer_purchase_count'}, inplace=True)
customers = customers.merge(recent_purchases, how='left', on='customer_id')

print("calculating user budget")
recent_customer_spendings = recent_slice.groupby('customer_id')[['price']].mean()
recent_customer_spendings.rename(columns={'price': 'average_customer_budget'}, inplace=True)
customers = customers.merge(recent_customer_spendings, how='left', on='customer_id')
customers.head()

print("calculating article purchase count")
recent_purchases = recent_slice.groupby('article_id')[['article_id']].count()
recent_purchases.rename(columns={'article_id': 'article_purchase_count'}, inplace=True)
articles = articles.merge(recent_purchases, how='left', on='article_id')

print("calculating average article price")
mean_article_prices = transactions.groupby('article_id')[['price']].mean()
mean_article_prices.rename(columns={'price': 'average_article_price'}, inplace=True)
articles = articles.merge(mean_article_prices, how='left', on='article_id')
del recent_slice
customers = customers.fillna(0)
articles = articles.fillna(0)
gc.collect()

articles.head()

"""Construct dataset, in same manner as shown in previous lecture. Includes creating negative samples"""

print("removing oldest data from transactions")
n_weeks = 20
last_day = transactions['t_dat'].max()
last_day_date = [int(i) for i in last_day.split('-')]
last_day_date = datetime.date(last_day_date[0], last_day_date[1], last_day_date[2])
min_date = last_day_date - datetime.timedelta(weeks=n_weeks)
min_date = min_date.strftime('%Y-%m-%d')
transactions = transactions[transactions['t_dat'] >= min_date]
gc.collect()


print("generating negative samples")
# negative sample creation
positive_pairs = list(map(tuple, transactions[['customer_id', 'article_id']].drop_duplicates().values))

real_dates = transactions["t_dat"].unique()
real_customers = transactions["customer_id"].unique()
real_articles = transactions["article_id"].unique()
real_channels = transactions["sales_channel_id"].unique()
article_and_price = transactions[["article_id","price"]].drop_duplicates("article_id").set_index("article_id").squeeze()
num_neg_pos = transactions.shape[0]
random.seed(42)
num_neg_samples = int(num_neg_pos * 1.1)
neg_dates = np.random.choice(real_dates, size=num_neg_samples)
neg_articles = np.random.choice(real_articles, size=num_neg_samples)
neg_customers = np.random.choice(real_customers, size=num_neg_samples)
neg_channels = np.random.choice(real_channels, size=num_neg_samples)
ordered = np.array([0] * num_neg_samples)
neg_prices = article_and_price[neg_articles].values
neg_transactions = pd.DataFrame([neg_dates, neg_customers, neg_articles, neg_prices, neg_channels, ordered], index=transactions.columns).T
df = neg_transactions[
    ~neg_transactions.set_index(["customer_id", "article_id"]).index.isin(positive_pairs)
]
chosen_neg_transactions = df.sample(num_neg_pos)
transactions = pd.concat([transactions, chosen_neg_transactions])
transactions = transactions.merge(customers, how="inner", on='customer_id')
transactions = transactions.merge(articles, how="inner", on='article_id')
transactions["price_discrepancy"] = transactions["average_article_price"] - transactions["average_customer_budget"]
del neg_transactions
print("slicing processed transactions")
transactions_processed = transactions[['customer_id', 'age', 'sales_channel_id', 'article_id', 'price', 'ordered', 'product_type_no', 
                                       'customer_purchase_count', 'article_purchase_count', 
                                       'average_customer_budget', 'average_article_price', 'price_discrepancy']].copy()
transactions_processed.head()
del transactions
gc.collect()
transactions_processed = transactions_processed.fillna(0)
transactions_processed.isnull().values.any()
transactions_processed = pd.get_dummies(transactions_processed, columns=['sales_channel_id'])

print("writing processed transactions to csv")
# write transactions_processed to csv so it can be re-used
processed_filename = "../data/transactions_processed.csv"
transactions_processed.to_csv(processed_filename, index=False)

# if file has been generated previously, can be retrieved from cache here
transactions_processed = pd.read_csv(processed_filename)

"""Gathering most popular items as baseline candidate set for each customer. Later to be padded with more personalized articles"""

print('generating global candidates')
most_popular_count = 20
popular_items = transactions_processed[transactions_processed['ordered']==1].drop_duplicates(subset='article_id')
popular_items.sort_values(by='article_purchase_count', ascending=False, inplace=True)
popular_items.drop(['customer_id', 'ordered', 'customer_purchase_count', 'average_customer_budget', 'price_discrepancy', 'age'], axis=1, inplace=True)
popular_items = popular_items.iloc[:most_popular_count]
# print(popular_items)

"""Deploying the model, first training on the entire dataset, then generating candidates for each customer and generating predictions."""

print("training model")
X_train, X_test, y_train, y_test = train_test_split(transactions_processed.drop('ordered', axis=1), transactions_processed['ordered'], test_size=0.10, random_state=42)
# model = MLPClassifier(hidden_layer_sizes=(9, 20, 9), max_iter=300, random_state=42) #9 20 9
model = DecisionTreeClassifier(random_state=42)
model = model.fit(X_train, y_train)
predictions = model.predict_proba(X_test)
print("validation score:", model.score(X_test, y_test))

"""Personalized candidate generation, based on:


*   Previously bought items
*   articles within specific price range from habitual spendings
*   popular items


"""

print("generating personalised candidates")
print(len(customers))
customer_temp = customers[['customer_id', 'average_customer_budget', 'customer_purchase_count', 'age']].drop_duplicates()
print('len', len(customer_temp))
# print(customer_ids)
candidate_pairs = customer_temp.merge(popular_items, how='cross')
candidate_pairs['price_discrepancy'] = candidate_pairs["average_article_price"] - candidate_pairs["average_customer_budget"]
candidate_pairs = candidate_pairs.reindex(columns=X_train.columns)

candidate_pairs.head()

print("predicting recommendations")
from copy import deepcopy
temp = deepcopy(candidate_pairs)
temp = temp.fillna(0)
temp[["prediction0", 'prediction']] = model.predict_proba(temp)
temp = temp[["customer_id", "article_id", "prediction"]]
temp.sort_values(['customer_id', 'prediction'], ascending=False, inplace=True)
temp["customer_id"] = customer_encoder.inverse_transform(temp['customer_id'])

print("writing recommendations")
predictions_file = "predictions.csv"
recommendation_count = 12
prediction_df = temp.groupby('customer_id').head(recommendation_count)
prediction_df['article_id'] = article_encoder.inverse_transform(prediction_df['article_id'])
prediction_df['article_id'] = prediction_df['article_id'].apply(str)
prediction_df["article_id"] = "0" + prediction_df["article_id"]
prediction_df.groupby('customer_id').agg({'article_id': " ".join}).to_csv(predictions_file)