# -*- coding: utf-8 -*-
"""cold_start_test

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1HpiKfjqcvCbwJMlcGxS5Tdh56NhcX0Sx
"""

import pandas as pd
import numpy as np
import random
import datetime
from sklearn import preprocessing
from sklearn.linear_model import LogisticRegression
from sklearn.neural_network import MLPClassifier, MLPRegressor
import lightgbm as lgbm
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report
import gc

articles_filename = '../data/articles.csv'
customers_filename = '../data/customers.csv'
transactions_filename = '../data/transactions_train.csv'

gc.collect()
print("reading files")
articles = pd.read_csv(articles_filename)
customers = pd.read_csv(customers_filename)
transactions = pd.read_csv(transactions_filename)
customer_encoder = preprocessing.LabelEncoder()
article_encoder = preprocessing.LabelEncoder()
customers['customer_id'] = customer_encoder.fit_transform(customers['customer_id'])
articles['article_id'] = article_encoder.fit_transform(articles['article_id'])
transactions['customer_id'] = customer_encoder.transform(transactions['customer_id'])
transactions['article_id'] = article_encoder.transform(transactions['article_id'])
transactions['ordered'] = 1

print("taking recent slice")
n_weeks = 4
last_day = transactions['t_dat'].max()
last_day_date = [int(i) for i in last_day.split('-')]
last_day_date = datetime.date(last_day_date[0], last_day_date[1], last_day_date[2])
min_date = last_day_date - datetime.timedelta(weeks=n_weeks)
min_date = min_date.strftime('%Y-%m-%d')
recent_slice = transactions[transactions['t_dat'] >= min_date].copy()

print("calculating user purchase count")
recent_purchases = recent_slice.groupby('customer_id')[['article_id']].count()
recent_purchases.rename(columns={'article_id': 'customer_purchase_count'}, inplace=True)
customers = customers.merge(recent_purchases, how='left', on='customer_id')

print("calculating user budget")
recent_customer_spendings = recent_slice.groupby('customer_id')[['price']].mean()
recent_customer_spendings.rename(columns={'price': 'average_customer_budget'}, inplace=True)
customers = customers.merge(recent_customer_spendings, how='left', on='customer_id')
customers.head()

print("calculating article purchase count")
recent_purchases = recent_slice.groupby('article_id')[['article_id']].count()
recent_purchases.rename(columns={'article_id': 'article_purchase_count'}, inplace=True)
articles = articles.merge(recent_purchases, how='left', on='article_id')

print("calculating average article price")
mean_article_prices = transactions.groupby('article_id')[['price']].mean()
mean_article_prices.rename(columns={'price': 'average_article_price'}, inplace=True)
articles = articles.merge(mean_article_prices, how='left', on='article_id')
customers = customers.fillna(0)
articles = articles.fillna(0)
gc.collect()
articles.head()

print("determining popular first items")
first_purchases = transactions.sort_values(by="t_dat").groupby('customer_id').first()
first_purchase_counts = first_purchases["article_id"].value_counts()
first_purchase_df = pd.DataFrame(data={'article_id':first_purchase_counts.index, 'first_purchase_count':first_purchase_counts.values})
articles = articles.merge(first_purchase_df, how="left")
articles = articles.fillna(0)

print("removing oldest data from transactions")
n_weeks = 20
if n_weeks >= 0:
  last_day = transactions['t_dat'].max()
  last_day_date = [int(i) for i in last_day.split('-')]
  last_day_date = datetime.date(last_day_date[0], last_day_date[1], last_day_date[2])
  min_date = last_day_date - datetime.timedelta(weeks=n_weeks)
  min_date = min_date.strftime('%Y-%m-%d')
  transactions = transactions[transactions['t_dat'] >= min_date]
gc.collect()

"""Activity-based features, for each customer: determine how many times they bought in the last n weeks"""

activity_features = []
offsets = [0]
print("calculating customer activity")
step_size = 2
for recent_weeks in range(1, 21, step_size):
  col_name = "activity_" + str(recent_weeks) + "_weeks"
  last_day = transactions['t_dat'].max()
  last_day_date = [int(i) for i in last_day.split('-')]
  last_day_date = datetime.date(last_day_date[0], last_day_date[1], last_day_date[2])
  min_date = last_day_date - datetime.timedelta(weeks=recent_weeks)
  min_date = min_date.strftime('%Y-%m-%d')
  recent_view = transactions['t_dat'] >= min_date
  customer_counts = transactions[recent_view]['customer_id'].value_counts()
  temp_df = pd.DataFrame(data={
      'customer_id': customer_counts.index,
      col_name: customer_counts.values
  })
  customers = customers.merge(temp_df, how='left')
  customers = customers.fillna(0)
  customers[col_name] -= offsets[-1]
  offsets = [customers[col_name]]
  activity_features.append(col_name)
del offsets

if len(activity_features) > 0:
  customers['recently_active'] = customers[activity_features[0]] > 0
  activity_features.append('recently_active')

# https://github.com/radekosmulski/personalized_fashion_recs/blob/main/01_Solution_warmup.ipynb
transactions.t_dat = pd.to_datetime(transactions.t_dat, format='%Y-%m-%d')
transactions.t_dat = 104 - (transactions.t_dat.max() - transactions.t_dat).dt.days

print("generating negative samples")
# negative sample creation
positive_pairs = list(map(tuple, transactions[['customer_id', 'article_id']].drop_duplicates().values))

real_dates = transactions["t_dat"].unique()
real_customers = transactions["customer_id"].unique()
real_articles = transactions["article_id"].unique()
real_channels = transactions["sales_channel_id"].unique()
article_and_price = transactions[["article_id","price"]].drop_duplicates("article_id").set_index("article_id").squeeze()
num_neg_pos = transactions.shape[0]
random.seed(42)
num_neg_samples = int(num_neg_pos * 1.1)
neg_dates = np.random.choice(real_dates, size=num_neg_samples)
neg_articles = np.random.choice(real_articles, size=num_neg_samples)
neg_customers = np.random.choice(real_customers, size=num_neg_samples)
neg_channels = np.random.choice(real_channels, size=num_neg_samples)
ordered = np.array([0] * num_neg_samples)
neg_prices = article_and_price[neg_articles].values
neg_transactions = pd.DataFrame([neg_dates, neg_customers, neg_articles, neg_prices, neg_channels, ordered], index=transactions.columns).T
df = neg_transactions[
    ~neg_transactions.set_index(["customer_id", "article_id"]).index.isin(positive_pairs)
]
chosen_neg_transactions = df.sample(num_neg_pos)
transactions = pd.concat([transactions, chosen_neg_transactions])
transactions = transactions.merge(customers, how="inner", on='customer_id')
transactions = transactions.merge(articles, how="inner", on='article_id')
transactions["price_discrepancy"] = transactions["average_article_price"] - transactions["average_customer_budget"]
del neg_transactions

print("slicing processed transactions")
feature_names = ['customer_id', 'age', 'sales_channel_id', 'article_id', 'price', 'ordered', 
            'product_type_no', 'customer_purchase_count', 'article_purchase_count', 
            'average_customer_budget', 'average_article_price', 'price_discrepancy', 
            'first_purchase_count', 't_dat'] + activity_features
transactions_processed = transactions[feature_names].copy()
transactions_processed.head()
del transactions
gc.collect()
transactions_processed = transactions_processed.fillna(0)
transactions_processed.isnull().values.any()
transactions_processed = pd.get_dummies(transactions_processed, columns=['sales_channel_id'])

print("writing processed transactions to csv")
# write transactions_processed to csv so it can be re-used
# processed_filename = "../data/transactions_processed.csv"
# transactions_processed.to_csv(processed_filename, index=False)

# if file has been generated previously, can be retrieved from cache here
# transactions_processed = pd.read_csv(processed_filename)

print('generating global candidates')
most_popular_count = 20
popular_items = transactions_processed[transactions_processed['ordered']==1].drop_duplicates(subset='article_id')
popular_items.sort_values(by='article_purchase_count', ascending=False, inplace=True)
popular_items.drop(['customer_id', 'ordered', 'customer_purchase_count', 'average_customer_budget', 'price_discrepancy', 'age'], axis=1, inplace=True)
popular_items = popular_items.iloc[:most_popular_count]
popular_items.head()

print("training model")
y = transactions_processed["ordered"]
x = transactions_processed.drop('ordered', axis=1)
# model = RandomForestClassifier(random_state=42, n_estimators=5, verbose=2)
model = lgbm.LGBMClassifier()
model = model.fit(x, y)

print("generating personalised candidates")
customer_temp = customers[['customer_id', 'average_customer_budget', 'customer_purchase_count', 'age']].drop_duplicates()
candidate_pairs = customer_temp.merge(popular_items, how='cross')
candidate_pairs['price_discrepancy'] = candidate_pairs["average_article_price"] - candidate_pairs["average_customer_budget"]
candidate_pairs = candidate_pairs.reindex(columns=x.columns)
candidate_pairs.head()

print("predicting recommendations")
candidate_pairs = candidate_pairs.fillna(0)
candidate_pairs[["prediction0", 'prediction']] = model.predict_proba(candidate_pairs)
candidate_pairs = candidate_pairs[["customer_id", "article_id", "prediction"]]
candidate_pairs.sort_values(['customer_id', 'prediction'], ascending=False, inplace=True)
candidate_pairs["customer_id"] = customer_encoder.inverse_transform(candidate_pairs['customer_id'])

candidate_pairs.groupby('customer_id').head(12)

print("writing recommendations")
predictions_file = "predictions.csv"
recommendation_count = 12
prediction_df = candidate_pairs.groupby('customer_id').head(recommendation_count)
print(prediction_df.head())
prediction_df['article_id'] = prediction_df['article_id'].apply(int)
print(prediction_df.head())
prediction_df['article_id'] = article_encoder.inverse_transform(prediction_df['article_id'])
prediction_df['prediction'] = prediction_df['article_id'].apply(str)
prediction_df["prediction"] = "0" + prediction_df["prediction"]
prediction_df.groupby('customer_id').agg({'prediction': " ".join}).to_csv(predictions_file)
