# -*- coding: utf-8 -*-
"""candidate_generation.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1gjRqYnZo92-_eh9bg6cl4XLPp1fTmmcg
"""

import pandas as pd
from efficient_apriori import apriori
import numpy as np
import random
import json
import multiprocessing as mp

######################################################################################################

settings_file = "./settings.json"
settings = json.load(open(settings_file))
random.seed(42)

# read files from settings
data_dir = settings["data_directory"]
processed_filenames = settings["data_filenames"]["processed"]
candidate_filenames = settings["candidate_filenames"]
transactions = pd.read_csv(data_dir + processed_filenames["transactions"])
transactions = transactions[transactions["ordered"] == 1]
bestsellers = pd.read_csv(data_dir + processed_filenames["bestsellers"])
articles = pd.read_csv(data_dir + processed_filenames["articles"])

"""
Slicing transactions to recent slice, in order to have more relevant information
"""
candidate_columns = transactions.columns
print("taking recent slice")
full_weeks = settings["full_weeks"]
recent_weeks = settings["recent_weeks"]
max_week = transactions["t_dat"].max() + 1
transactions = transactions[(max_week - transactions["t_dat"]) <= recent_weeks]
bestsellers = bestsellers[bestsellers["t_dat"] == max_week]

"""
Baskets are the set of transactions each user has made across the timeframe,
does not keep track of order or re-purchases
"""
print("generating baskets")
baskets = transactions.groupby("customer_id")["article_id"].apply(set).values
baskets = baskets.tolist()

"""
Parameters for the library are slightly different
support is scaled by the amount of baskets in the dataset,
meaning a min_support of 0.1 requires an item set to appear in 10% of all baskets
The settings file specifies a flat number however which has to be scaled 
"""
print("running apriori")
min_support = settings["itemset_params"]["min_support"]
min_support = min_support / len(baskets)
nr_of_transactions = len(transactions)
"""
Recommendations should at least be equally confident as randomly recommending
the most popular item
"""
min_confidence = transactions.article_purchase_count.max() / len(baskets)

line_items = 0
customer_counter = 0
# reset transactions to full length for generating baskets
# I am not sure which was better
transaction_filename = data_dir + processed_filenames["transactions"]
transactions = pd.read_csv(transaction_filename)
candidate_df = pd.DataFrame(columns=candidate_columns)
candidate_df["recently_active"] = candidate_df["recently_active"].astype("bool")
neg_candidates = pd.DataFrame(columns=candidate_columns)
cid = transactions["customer_id"].unique()
customer_chunks = pd.read_csv(data_dir + processed_filenames["customers"])
customer_chunks = np.array_split(customer_chunks, 5)
candidate_count = settings["itemset_params"]["candidate_count"]
file_counter = 0

########################################################################


def run_apriori(baskets, min_support, min_confidence):
    """
    Calls the apriori algorithm from the library, using specified
    parameters
    :param baskets: a transaction set of each customer
    :param min_support: the minimum support for each rule, scaled to dataset size
    :param min_confidence: the minimum confidence in each rule
    :return: a list of association rules, ordered by decreasing confidence
    """
    print("using min_support of", str(min_support))
    print("using min_confidence of", str(min_confidence))
    itemset, result = list(apriori(baskets, min_confidence=min_confidence,
                          min_support=min_support,
                                   verbosity=1))

    result.sort(key=lambda x: x.support, reverse=True)
    # arbitrarily take top 1% frequent itemsets
    # result = result[:math.ceil(len(result) / 10)]

    print("extracting rules")
    rules = {}
    for rule in result:
        if len(rule.lhs) > 0:
            if rule.lhs not in rules.keys():
                rules[rule.lhs] = []
            rules[rule.lhs] += [item for item in rule.rhs]
    print("generated", len(rules.keys()), "rules")
    return rules


def get_recommendations(itemset: set, max_count: int, rules):
    """
    For a set of items, generate a list of recommendations,
    if a rule would cause the recommendation count to exceed max_count,
    a random sample is chosen to recommend
    :param itemset: the previously bought items
    :param max_count: the maximum amount of items to recommend
    :param rules: the association rules generated from apriori
    :return: a set of items to recommend a user
    """
    recs = set()
    for key in rules.keys():
        precedent = set(key)
        consequent = set(rules[key])
        if precedent.issubset(itemset):
            recs = recs.union(consequent)
            # prevent already bought items from being recommended here
            recs = recs.difference(itemset)
            if len(recs) >= max_count:
                return random.sample(recs, max_count)
    return recs


def handle_customers(chunks, i, candidate_count, rules):
    """
    Multiprocessing function,
    takes the i'th element of the customer chunks, and generates recommendations
    for each candidate

    The function keeps track of an intermediate dataframe holding all candidates
    Once this exceeds a certain size the dataframe is written to a candidate file,
    and also concatenated to a negative samples dataframe
    :param chunks: a list of chunks of customer entries
    :param i: the index of the chunk to handle within this process
    :param candidate_count: the amount of items to recommend per user
    :param rules: association rules previously generated
    :return: None
    """
    global candidate_df
    customers = chunks[i]
    file_counter = 0
    customers = customers[customers["customer_id"].isin(cid)].copy()
    customers["recently_active"] = customers["recently_active"].apply(bool)
    print("generating baskets per user for batch", i, flush=True)
    customers["baskets"] = transactions[transactions["ordered"]==1].groupby("customer_id")["article_id"].apply(set)
    customers["baskets"] = customers["baskets"].fillna("").apply(set)

    def write_df(df: pd.DataFrame):
        """
        writes the internal df and concatenates it to the negative samples
        :param df: a dataframe holding candidate transactions
        :return: None
        """
        global neg_candidates
        global file_counter
        candidate_dir = settings["candidate_directory"]
        temp_filename = candidate_dir + str(file_counter) + "_" + str(i) + settings["candidate_filenames"]["itemsets"]
        df.to_csv(temp_filename, index=False)
        file_counter += 1
        # add negative samples
        df["t_dat"] -= 1
        df["ordered"] = 0
        print('writing candidates file')
        neg_candidates = pd.concat([neg_candidates, df])
        neg_candidates.drop_duplicates(subset=["customer_id", "article_id"], inplace=True)

    def generate_recs(customer_id, done_count):
        """
        For a given customer id, determine the recommendations based on prior purchases
        :param customer_id:
        :param done_count:
        :return:
        """
        global candidate_df
        global neg_candidates
        row = customers[customers["customer_id"] == customer_id].copy()
        customer_baskets = row.iloc[0].baskets
        if len(customer_baskets) > 0:
            article_view = articles["article_id"].isin(list(customer_baskets))
            temp = articles[article_view].copy()
            temp = row.merge(temp, how="cross")
            temp.drop(["baskets"], inplace=True, axis=1)
            temp["price_discrepancy"] = temp["average_article_price"] - temp["average_customer_budget"]
            temp["t_dat"] = max_week + 1
            temp["price"] = temp["average_article_price"]
            temp["sales_channel_id"] = 2
            temp = temp.merge(bestsellers, how="left", on=["t_dat", "article_id"])
            temp["bestseller_rank"].fillna(999, inplace=True)
            temp = temp.reindex(columns=candidate_columns)
            candidate_df = pd.concat([candidate_df, temp])
            if len(candidate_df) > 300000:
                write_df(candidate_df)
                candidate_df = pd.DataFrame(columns=candidate_columns)

        print(f"done with {done_count} customers in cluster")

    print("generating recommendations per user")
    customers["baskets"] = customers["baskets"].apply(lambda x: get_recommendations(x, candidate_count, rules))

    print("writing all recommendations to file")
    customers["customer_id"].apply(generate_recs)
    # at the end, write remaining entries to last csv
    if len(candidate_df) > 0:
        write_df(candidate_df)
        candidate_df = pd.DataFrame(columns=candidate_columns)


def add_negative_samples(transactions, neg_candidates):
    """
    concatenates negative samples generated from association rules to the
    original transactions dataframe, then writes to original file
    :param transactions:
    :param neg_candidates:
    :return:
    """
    print("adding negative samples to last training week")
    transactions = pd.concat([transactions, neg_candidates])
    transactions.drop_duplicates(subset=["customer_id", "article_id", "t_dat"], inplace=True)
    transactions.to_csv(transaction_filename, index=False)


if __name__ == "__main__":
    rules = run_apriori(baskets, min_support, min_confidence)
    mp.freeze_support()
    for i in range(len(customer_chunks)):
        # handle_customers(i, candidate_count, rules)
        p = mp.Process(target=handle_customers, args=(customer_chunks, i, candidate_count, rules))
        p.start()
