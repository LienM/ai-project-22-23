# -*- coding: utf-8 -*-
"""candidate_generation.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1gjRqYnZo92-_eh9bg6cl4XLPp1fTmmcg
"""

import pandas as pd
from efficient_apriori import apriori
import numpy as np
import random
import datetime
import json
import math
import multiprocessing as mp
from sklearn import preprocessing
import lightgbm as lgbm
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report
import gc

######################################################################################################

settings_file = "./settings.json"
settings = json.load(open(settings_file))
random.seed(42)

# read file from settings
data_dir = settings["data_directory"]
processed_filenames = settings["data_filenames"]["processed"]
candidate_filenames = settings["candidate_filenames"]
transactions = pd.read_csv(data_dir + processed_filenames["transactions"])
transactions = transactions[transactions["ordered"] == 1]
bestsellers = pd.read_csv(data_dir + processed_filenames["bestsellers"])
articles = pd.read_csv(data_dir + processed_filenames["articles"])

candidate_columns = transactions.columns
print("taking recent slice")
full_weeks = settings["full_weeks"]
recent_weeks = settings["recent_weeks"]
max_week = transactions["t_dat"].max() + 1
transactions = transactions[(max_week - transactions["t_dat"]) <= recent_weeks]
bestsellers = bestsellers[bestsellers["t_dat"] == max_week]

print("generating baskets")
baskets = transactions.groupby("customer_id")["article_id"].apply(set).values
baskets = baskets.tolist()
print("running apriori")
min_support = settings["itemset_params"]["min_support"]
min_support = min_support / len(baskets)
nr_of_transactions = len(transactions)
min_confidence = transactions.article_purchase_count.max() / len(baskets)

line_items = 0
customer_counter = 0
# reset transactions to full length for generating baskets
transaction_filename = data_dir + processed_filenames["transactions"]
transactions = pd.read_csv(transaction_filename)
candidate_df = pd.DataFrame(columns=candidate_columns)
candidate_df["recently_active"] = candidate_df["recently_active"].astype("bool")
neg_candidates = pd.DataFrame(columns=candidate_columns)
cid = transactions["customer_id"].unique()
customer_chunks = pd.read_csv(data_dir + processed_filenames["customers"])
customer_chunks = np.array_split(customer_chunks, 5)
candidate_count = settings["itemset_params"]["candidate_count"]
file_counter = 0

########################################################################


def run_apriori(baskets, min_support, min_confidence):
    print("using min_support of", str(min_support))
    print("using min_confidence of", str(min_confidence))
    itemset, result = list(apriori(baskets, min_confidence=min_confidence,
                          min_support=min_support,
                                   verbosity=1))

    result.sort(key=lambda x: x.support, reverse=True)
    # arbitrarily take top 1% frequent itemsets
    # result = result[:math.ceil(len(result) / 10)]

    print("extracting rules")
    rules = {}
    for rule in result:
        if len(rule.lhs) > 0:
            if rule.lhs not in rules.keys():
                rules[rule.lhs] = []
            rules[rule.lhs] += [item for item in rule.rhs]
    print("generated", len(rules.keys()), "rules")
    return rules


def get_recommendations(itemset: set, max_count: int, rules):
    recs = set()
    for key in rules.keys():
        precedent = set(key)
        consequent = set(rules[key])
        if precedent.issubset(itemset):
            recs = recs.union(consequent)
            # prevent already bought items from being recommended here
            recs = recs.difference(itemset)
            if len(recs) >= max_count:
                return random.sample(recs, max_count)
    return recs


def handle_customers(chunks, i, candidate_count, rules):
    global candidate_df
    done_count = 0
    customers = chunks[i]
    file_counter = 0
    customers = customers[customers["customer_id"].isin(cid)].copy()
    customers["recently_active"] = customers["recently_active"].apply(bool)
    print("generating baskets per user for batch", i, flush=True)
    customers["baskets"] = transactions[transactions["ordered"]==1].groupby("customer_id")["article_id"].apply(set)
    customers["baskets"] = customers["baskets"].fillna("").apply(set)

    def write_df(df: pd.DataFrame):
        global neg_candidates
        global file_counter
        candidate_dir = settings["candidate_directory"]
        temp_filename = candidate_dir + str(file_counter) + "_" + str(i) + settings["candidate_filenames"]["itemsets"]
        df.to_csv(temp_filename, index=False)
        file_counter += 1
        # add negative samples
        df["t_dat"] -= 1
        df["ordered"] = 0
        print('writing candidates file')
        neg_candidates = pd.concat([neg_candidates, df])
        neg_candidates.drop_duplicates(subset=["customer_id", "article_id"], inplace=True)

    def generate_recs(customer_id, done_count):
        global candidate_df
        global neg_candidates
        row = customers[customers["customer_id"] == customer_id].copy()
        customer_baskets = row.iloc[0].baskets
        if len(customer_baskets) > 0:
            article_view = articles["article_id"].isin(list(customer_baskets))
            temp = articles[article_view].copy()
            temp = row.merge(temp, how="cross")
            temp.drop(["baskets"], inplace=True, axis=1)
            temp["price_discrepancy"] = temp["average_article_price"] - temp["average_customer_budget"]
            temp["t_dat"] = max_week + 1
            temp["price"] = temp["average_article_price"]
            temp["sales_channel_id"] = 2
            temp = temp.merge(bestsellers, how="left", on=["t_dat", "article_id"])
            temp["bestseller_rank"].fillna(999, inplace=True)
            temp = temp.reindex(columns=candidate_columns)
            candidate_df = pd.concat([candidate_df, temp])
            if len(candidate_df) > 300000:
                write_df(candidate_df)
                candidate_df = pd.DataFrame(columns=candidate_columns)

        print(f"done with {done_count} customers in cluster")

    print("generating recommendations per user")
    customers["baskets"] = customers["baskets"].apply(lambda x: get_recommendations(x, candidate_count, rules))

    print("writing all recommendations to file")
    customers["customer_id"].apply(generate_recs)
    if len(candidate_df) > 0:
        write_df(candidate_df)
        candidate_df = pd.DataFrame(columns=candidate_columns)


def add_negative_samples(transactions, neg_candidates):
    print("adding negative samples to last training week")
    transactions = pd.concat([transactions, neg_candidates])
    transactions.drop_duplicates(subset=["customer_id", "article_id", "t_dat"], inplace=True)
    transactions.to_csv(transaction_filename, index=False)


if __name__ == "__main__":
    rules = run_apriori(baskets, min_support, min_confidence)
    mp.freeze_support()
    for i in range(len(customer_chunks)):
        # handle_customers(i, candidate_count, rules)
        p = mp.Process(target=handle_customers, args=(customer_chunks, i, candidate_count, rules))
        p.start()
