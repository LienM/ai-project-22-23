{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7adaeed1",
   "metadata": {},
   "source": [
    "# Sales Channels\n",
    "Objectives:\n",
    "- Try to figure out if the kaggle contest assumes only online (type 2) transactions are relevant\n",
    "\n",
    "Method:\n",
    "- Include sales channel when training model (LightGBM used)\n",
    "- Use only popular items from online sales channel for candidate generation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8d75830a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd\n",
    "import random\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn import preprocessing\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3a8d3320",
   "metadata": {},
   "outputs": [],
   "source": [
    "transactions = pd.read_csv('./data/transactions_train.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dba497b",
   "metadata": {},
   "source": [
    "Replace customer ids with label encoding for space preservation purposes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "836ef65e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_transactions_customers(transactions_df):\n",
    "    customers = pd.read_csv('./data/customers.csv')\n",
    "    customer_encoder = preprocessing.LabelEncoder()\n",
    "    customer_encoder.fit(customers['customer_id'])\n",
    "    transactions['customer_id'] = customer_encoder.transform(transactions['customer_id'])\n",
    "    np.save('customer_ids.npy', customer_encoder.classes_) \n",
    "    return customer_encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "535b1c84",
   "metadata": {},
   "outputs": [],
   "source": [
    "customer_encoder = encode_transactions_customers(transactions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0c51ad8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "transactions['purchased'] = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53aecf7b",
   "metadata": {},
   "source": [
    "Transform string dates into weeks with the start of week 0 being the week for the final calculation (meaning everything in the data becomes a negative week)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c5515a8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_string_dates_to_int(transactions_df):\n",
    "    import datetime\n",
    "\n",
    "    lookup = dict()\n",
    "    def str_dat_to_weeks_int(datestring):\n",
    "        return lookup.setdefault(datestring, (datetime.datetime.strptime(datestring, \"%Y-%m-%d\") - datetime.datetime(2020, 9, 23)).days//7)\n",
    "    \n",
    "    transactions_df[\"t_dat\"] = transactions_df[\"t_dat\"].map(str_dat_to_weeks_int)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6bd2c916",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform_string_dates_to_int(transactions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8e79330",
   "metadata": {},
   "source": [
    "Drop all transactions which happened more than 20 weeks before the end of the data collection period"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "12e1ab89",
   "metadata": {},
   "outputs": [],
   "source": [
    "transactions.drop(transactions[transactions[\"t_dat\"] < -20].index, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31c989b0",
   "metadata": {},
   "source": [
    "Perform random negative sampling, most of this code is copied from the 2nd lecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f2c8a676",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_negative_samples_np_version(transactions_df, num_neg_pos):\n",
    "    real_dates = transactions[\"t_dat\"].unique()\n",
    "    real_customers = transactions[\"customer_id\"].unique()\n",
    "    real_articles = transactions[\"article_id\"].unique()\n",
    "    real_channels = transactions[\"sales_channel_id\"].unique()\n",
    "    article_and_price = transactions[[\"article_id\",\"price\"]].drop_duplicates(\"article_id\").set_index(\"article_id\").squeeze()\n",
    "\n",
    "    random.seed(42)\n",
    "    num_neg_samples = int(num_neg_pos * 1.1)\n",
    "\n",
    "    neg_dates = np.random.choice(real_dates, size=num_neg_samples)\n",
    "    neg_articles = np.random.choice(real_articles, size=num_neg_samples)\n",
    "    neg_customers = np.random.choice(real_customers, size=num_neg_samples)\n",
    "    neg_channels = np.random.choice(real_channels, size=num_neg_samples)\n",
    "    ordered = np.array([0] * num_neg_samples)\n",
    "\n",
    "    neg_prices = article_and_price[neg_articles].values\n",
    "    \n",
    "    return np.column_stack((neg_dates, neg_customers, neg_articles, neg_prices, neg_channels, ordered))\n",
    "    \n",
    "def generate_negative_samples(transactions_df):\n",
    "    num_neg_pos = transactions_df.shape[0]\n",
    "    positive_pairs = list(map(tuple, transactions_df[['customer_id', 'article_id']].drop_duplicates().values))\n",
    "    neg_transactions = pd.DataFrame(generate_negative_samples_np_version(transactions_df, num_neg_pos), columns=transactions_df.columns)\n",
    "    duplicate_indexes = neg_transactions[[\"customer_id\", \"article_id\"]].apply(tuple, 1).isin(positive_pairs)\n",
    "    neg_transactions = neg_transactions[~duplicate_indexes]\n",
    "    return neg_transactions.sample(num_neg_pos)\n",
    "\n",
    "def negative_sampling(transactions_df):\n",
    "    transactions_df = pd.concat([transactions_df, generate_negative_samples(transactions_df)])\n",
    "    transactions_df.reset_index(drop=True, inplace=True)\n",
    "    return transactions_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "80eec622",
   "metadata": {},
   "outputs": [],
   "source": [
    "transactions = negative_sampling(transactions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8364c64b",
   "metadata": {},
   "outputs": [],
   "source": [
    "transactions.to_feather('./data/negativesampled.feather')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5633388d",
   "metadata": {},
   "source": [
    "This is intended as a checkpoint for if I need to reset the jupyter notebook kernel for whatever reason. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "86d97c15",
   "metadata": {},
   "outputs": [],
   "source": [
    "articles = pd.read_csv('./data/articles.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b5f42da1",
   "metadata": {},
   "outputs": [],
   "source": [
    "transactions = pd.read_feather(\"./data/negativesampled.feather\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "df586f9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "customers = pd.read_csv('./data/customers.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "16d3b561",
   "metadata": {},
   "outputs": [],
   "source": [
    "customer_encoder = preprocessing.LabelEncoder()\n",
    "customer_encoder.classes_ = np.load(\"customer_ids.npy\", allow_pickle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c499d39",
   "metadata": {},
   "source": [
    "Apply the label encoding to the customer table so it can be joined with transactions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "df8501e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "customers['customer_id'] = customer_encoder.transform(customers['customer_id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a4096f15",
   "metadata": {},
   "outputs": [],
   "source": [
    "zip_encoder = preprocessing.LabelEncoder()\n",
    "customers[\"postal_code\"] = zip_encoder.fit_transform(customers[\"postal_code\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "40f0e087",
   "metadata": {},
   "outputs": [],
   "source": [
    "def correct_types_and_merge_transactions(transactions_df, customers_df, articles_df):\n",
    "    customers_df[\"age\"] = customers_df[\"age\"].fillna(25)\n",
    "    customers_df[\"age\"] = customers_df[\"age\"].astype(int)\n",
    "    articles_df[['article_id', 'product_code', 'product_type_no','graphical_appearance_no','colour_group_code',\n",
    "       'perceived_colour_value_id', 'perceived_colour_master_id', 'department_no',\n",
    "       'index_group_no', 'section_no', 'garment_group_no']] = articles_df[['article_id', 'product_code',\n",
    "       'product_type_no','graphical_appearance_no','colour_group_code',\n",
    "       'perceived_colour_value_id', 'perceived_colour_master_id', 'department_no',\n",
    "       'index_group_no', 'section_no', 'garment_group_no']].astype(int)\n",
    "    transactions_df[['t_dat', 'customer_id', 'article_id', 'sales_channel_id', 'purchased']] = transactions_df[['t_dat', 'customer_id', 'article_id', 'sales_channel_id', 'purchased']].astype(int)\n",
    "\n",
    "    transactions_df = transactions_df.merge(customers_df[[\"customer_id\", \"age\", \"postal_code\"]], how=\"inner\", on='customer_id')\n",
    "    transactions_df = transactions_df.merge(articles_df[[\"article_id\", \"product_code\", \"product_type_no\", \"graphical_appearance_no\", \"colour_group_code\", \"department_no\", \"index_group_no\", \"section_no\", \"garment_group_no\"]], how=\"inner\", on='article_id')\n",
    "    return transactions_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "d7422747",
   "metadata": {},
   "outputs": [],
   "source": [
    "transactions = correct_types_and_merge_transactions(transactions, customers, articles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "32984d46",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(transactions.drop(['purchased', \"price\"], axis=1), transactions['purchased'], test_size=0.10, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd9268f0",
   "metadata": {},
   "source": [
    "Calculate the popular items which I will be using as candidates for the submission.\n",
    "\n",
    "One of the big changes from the other files occurs here: only items from sales channel 2 are considered for the popular candidates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "fb9997ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_popular_items(transaction_df):\n",
    "    popular_all_time = transaction_df[(transaction_df[\"purchased\"] == 1) & (transaction_df[\"sales_channel_id\"] == 2)][[\"article_id\", \"purchased\"]].groupby(\"article_id\").count().sort_values(ascending=False, by=\"purchased\").head(24).index.to_series().reset_index(drop=True)\n",
    "    popular_by_month = transaction_df[(transaction_df[\"purchased\"] == 1) & (transaction_df[\"t_dat\"] >= -4) & (transaction_df[\"sales_channel_id\"] == 2)][[\"article_id\", \"purchased\"]].groupby(\"article_id\").count().sort_values(ascending=False, by=\"purchased\").head(48).index.to_series().reset_index(drop=True)\n",
    "    popular_by_month2 = popular_by_month[~popular_by_month.isin(popular_all_time)]\n",
    "    popular_by_month2 = popular_by_month2.reset_index(drop=True).head(24)\n",
    "    popular_candidates = pd.DataFrame(pd.concat([popular_all_time, popular_by_month2])).astype(int).reset_index(drop=True)\n",
    "    return popular_candidates, popular_by_month\n",
    "\n",
    "popular_candidates, popular_by_month = get_popular_items(transactions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e85eb5d",
   "metadata": {},
   "source": [
    "Generate dataframe with all active customers and the candidates\n",
    "\n",
    "This is where one of the big changes from the other files occurs: sales_channel_id: 2 gets appended to all the candidates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "46e9286d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_ranker_input_df(transactions_df, candidates, customers_df, articles_df):\n",
    "    return pd.DataFrame(data={\"t_dat\": 0}, index=[0]).merge(transactions_df[[\"customer_id\"]].drop_duplicates(subset=\"customer_id\"), how=\"cross\").merge(candidates, how=\"cross\").merge(pd.DataFrame(data={\"sales_channel_id\": 2}, index=[0]), how=\"cross\").merge(customers_df[[\"customer_id\", \"age\", \"postal_code\"]], how=\"inner\", on=\"customer_id\").merge(articles_df[[\"article_id\", \"product_code\", \"product_type_no\", \"graphical_appearance_no\", \"colour_group_code\", \"department_no\", \"index_group_no\", \"section_no\", \"garment_group_no\"]], how=\"inner\", on=\"article_id\")\n",
    "\n",
    "ranker_input = generate_ranker_input_df(transactions, popular_candidates, customers, articles)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5e5a526",
   "metadata": {},
   "source": [
    "Functions to perform predictions and write to file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "b6e44175",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copied from the radekosmulski notebook\n",
    "def generate_ranking(df):\n",
    "    return df.sort_values(by=[\"customer_id\", \"p1\"], ascending=[True, False]).groupby(\"customer_id\")[\"article_id\"].apply(list).to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "147249d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def output_preds(df, classifier, filename, filler_candidates):\n",
    "    predictions = classifier.predict_proba(df)\n",
    "    df[[\"p0\", \"p1\"]] = predictions\n",
    "    ranking = generate_ranking(df)\n",
    "    \n",
    "    # Copied from the radekosmulski notebook\n",
    "    submission_df = pd.read_csv('data/sample_submission.csv')\n",
    "    preds = []\n",
    "    for c_id in customer_encoder.transform(submission_df.customer_id):\n",
    "        pred = ranking.get(c_id, filler_candidates)\n",
    "        preds.append(pred[:12])\n",
    "        \n",
    "    preds = [' '.join(['0' + str(p) for p in ps]) for ps in preds]\n",
    "    submission_df.prediction = preds\n",
    "    \n",
    "    submission_df.to_csv('./data/{}.csv'.format(filename), index=False)\n",
    "    df.drop(['p0', 'p1'], inplace=True, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "222bc88a",
   "metadata": {},
   "source": [
    "Let's start with lightgbm binary classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "dd342ff7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training...\n",
      "Training until validation scores don't improve for 5 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[20]\tvalid_0's auc: 0.820167\tvalid_0's binary_logloss: 0.533065\tvalid_0's l1: 0.382037\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LGBMClassifier(metric='l1', n_estimators=20)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# copying from https://github.com/microsoft/LightGBM/blob/master/examples/python-guide/simple_example.py\n",
    "# combined with https://github.com/angelotc/LightGBM-binary-classification-example/blob/master/CCData.ipynb\n",
    "\n",
    "import lightgbm as lgb\n",
    "print('Starting training...')\n",
    "\n",
    "gbm = lgb.LGBMClassifier(learning_rate = 0.1, metric = 'l1', \n",
    "                        n_estimators = 20)\n",
    "gbm.fit(X_train, y_train,\n",
    "        eval_set=[(X_test, y_test)],\n",
    "        eval_metric=['auc', 'binary_logloss'],\n",
    "        callbacks=[lgb.early_stopping(stopping_rounds=5)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "8c93263f",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_preds(ranker_input, gbm, \"sales_channel_2_only\", popular_by_month)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f942ec8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
